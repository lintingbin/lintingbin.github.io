{"meta":{"title":"Darcy's Blog","subtitle":"不如烂笔头","description":"欢迎来到我的个人站","author":"lintingbin2009","url":"https://lintingbin2009.github.io"},"pages":[{"title":"categories","date":"2017-04-29T05:54:12.000Z","updated":"2018-05-04T15:15:55.990Z","comments":false,"path":"categories/index.html","permalink":"https://lintingbin2009.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-04-29T05:53:57.000Z","updated":"2018-05-04T15:15:56.001Z","comments":false,"path":"tags/index.html","permalink":"https://lintingbin2009.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Spark报错——java.lang.outofmemoryerror: java heap space问题处理","slug":"Spark报错——java-lang-outofmemoryerror-java-heap-space问题处理","date":"2018-05-20T07:07:11.000Z","updated":"2018-05-20T08:05:24.364Z","comments":true,"path":"2018/05/20/Spark报错——java-lang-outofmemoryerror-java-heap-space问题处理/","link":"","permalink":"https://lintingbin2009.github.io/2018/05/20/Spark报错——java-lang-outofmemoryerror-java-heap-space问题处理/","excerpt":"","text":"最近在用spark处理数据的时候遇到内存不足的报错，主要的报错信息是在executor端的log中显示java.lang.outofmemoryerror: java heap space。 问题描述具体的问题是spark在执行到最后一个stage后有一个task一直执行不成功，每次都是重试四次后失败。下面的两张图是具体失败的信息： 四次task失败信息 具体失败的log task的失败的信息图中显示：失败的任务的Shffle Read Size是0，这个是不对的，因为这个信息在任务失败的时候都会被置零，实际上在任务在运行的时候这个值是六百多M，远远大于其他task的输入的20多M。 从上面失败的信息中我们可以看到失败的原因是有一个task的输入的数据量太大，以至于spark executor运行的时候需要的内存大大增加，这才导致了内存不足的异常。 问题解决解决尝试一最简单直接的解决方法是直接通过增大executor-memory的值来增加executor最大的内存使用量，由于yarn默认的每个executor的core是一个，如果本身启动的executor比较多的话，增加executor-memory的值的话，yarn集群就要多消耗executor的数量✖️增加的内存量的内存，内存的消耗会比较大。所以可以减少executor的数量，为每个executor分配多个core，这样需要的内存量就大大的减少了，但是每个executor可以使用的内存量又可以增加，这样的配置可以减少因为数据倾斜导致任务失败的概率。 最终我们用这个方法把每个executor的executor-memory值增大到了12G，但是最后还是由于内存不够失败了。 解决尝试二由于某个task需要的内存量非常的大，然而其他task的内存量都很小，这应该不是简单的数据倾斜。spark sql只是对玩家的登陆数据进行以device_id为key的group by操作，数据的倾斜不可能这么严重。 在重新观察了玩家的登陆数据后，我发现有很多数据的device_id为null。这下就很清楚的知道数据倾斜的原因了，接着对device_id为null的数据进行过滤后，问题就迎刃而解了。 总结在处理数据倾斜问题的时候可以通过调整spark的参数来优化任务的执行。但是如果想更彻底的优化任务的执行的话，要观察数据，知道是什么原因造成的数据倾斜。这样才能进行更彻底的优化。","categories":[{"name":"错误处理","slug":"错误处理","permalink":"https://lintingbin2009.github.io/categories/错误处理/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://lintingbin2009.github.io/tags/spark/"}]},{"title":"使用spark进行流处理","slug":"使用spark进行流处理","date":"2018-05-04T15:34:53.000Z","updated":"2018-05-04T16:57:24.473Z","comments":true,"path":"2018/05/04/使用spark进行流处理/","link":"","permalink":"https://lintingbin2009.github.io/2018/05/04/使用spark进行流处理/","excerpt":"","text":"最近在做一个假量检测的项目，主要是用来检测是否有一些伪造的广告点击之类的，然后该项目使用了spark来做在线的流处理 spark的使用场景spark主要用来读取kafka里面的一些点击、安装、登入和登出等数据，然后使用spark的流处理模块对这些数据进行处理，最后把处理完的数据存储到相应的数据库中，供后面的数据分析使用。 使用的spark流处理模块spark的流处理模块有两个： Spark Streaming(Dstream) 老的接口 Structured Streaming 新的接口 我们的项目使用了Dstream实现流处理，一个主要的原因是在新的Structured Streaming中我们不能获取到读取的kafka的offset，这样当我们有数据处理失败的时候就不能从相应的offset中恢复继续运行，虽然可以设置checkpoint来恢复失败的任务，但是checkpoint的恢复是基于任务的，不能对该任务进行修改，然后再重新运行。对于怎么在Structured Streaming中获取offset，我查了一些资料，如果实在是想获取offset的话也可以通过读取checkpoint文件夹下面的offset文件夹来获取当前的offset，不过这种方法比较奇怪。还有一个方法是使用StreamingQueryListener类里面的onQueryProgress回调来获取当前执行的状态，其中包括offset的信息，但是非常遗憾这种方法只支持scala和java，而我们的开发语言是python。下面的链接是该问题的具体讨论：如何从Structed streaming中获取offset的问题。 在一个流中处理多个topic感觉spark的api设置的非常不友好，想要在一个流中处理多个topic也挺麻烦的，主要的问题如下： 如果使用Dstream，在创建Dstream的时候可以传入多个topic，这样貌似可以解决读取多个topic的问题，但是有一个很严重的问题，读取到的内容你不知道是属于哪个topic，这样你就不能对不同的topic执行不同的处理了。 如果使用Structured Streaming，也可以在DataStreamReader中指定多个topic，而且传入的每行数据中也有相应的topic信息，是可以根据不同的topic来调用不同的处理方法的。但是如上面所说的，Structured Streaming不支持获取offset让我们放弃了它。 最后我们的处理方法是在一个流中建立多个Dstream，在每个Dstream中拉取和处理同一个topic的数据，这样一个流就可以处理多个topic了，示例代码如下所示： 123456for topic in topic_info: from_offsets = restore_off_sets(topic) DStream = KafkaUtils.createDirectStream(ssc, [topic], kafka_params, from_offsets) DStream.transform((lambda t: lambda rdd: get_offset_ranges(t, rdd))(topic))\\ .map(lambda x: x[1])\\ .foreachRDD((lambda t: lambda rdd: process_rdd(t, rdd))(topic)) 总结第一次使用spark，感觉spark的接口设置不是很友好，而且文档写的也不是很友好。比如foreachRDD的回调函数如果是两个参数的函数的话，第一个参数就是时间，这个在文档中没有提及，一不注意就有奇怪的bug了。总之自己还是一个菜鸟，还要多多学习。","categories":[{"name":"流处理","slug":"流处理","permalink":"https://lintingbin2009.github.io/categories/流处理/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://lintingbin2009.github.io/tags/spark/"}]},{"title":"Fluentd语法速记","slug":"fluentd语法速记","date":"2018-05-01T06:17:05.000Z","updated":"2018-05-04T15:15:55.985Z","comments":true,"path":"2018/05/01/fluentd语法速记/","link":"","permalink":"https://lintingbin2009.github.io/2018/05/01/fluentd语法速记/","excerpt":"","text":"最近开始转行做大数据，大数据中很重要的一部分是数据的收集，我们公司主要用的数据收集工具是Fluentd，由于Fluentd的配置比较多，有可能配置过一次后就会忘了。我这边在学习Fluentd配置的同时也对这些配置进行一些记录，方便后面再用到时可以快速的查找。 Fluentd简介Fluentd是一款完全免费且完全开源的日志收集器，拥有“Log Everything”的体系结构，能够与125种以上的系统对接。 Fluentd-architecture 配置文件语法Fluentd事件的生命周期 每个输入的事件会带有一个tag Fluentd通过tag匹配output Fluentd发送事件到匹配的output Fluentd支持多个数据源和数据输出 通过过滤器，事件可以被重新触发 “source”: 定义数据源数据源可以在source指令中定义，比如我们可以定义http和forward的数据源。http数据源可以通过http协议来接收数据，forward可以通过tcp协议来接收数据。 123456789101112# Receive events from 24224/tcp# This is used by log forwarding and the fluent-cat command&lt;source&gt; @type forward port 24224&lt;/source&gt;# http://this.host:9880/myapp.access?json=&#123;\"event\":\"data\"&#125;&lt;source&gt; @type http port 9880&lt;/source&gt; 所有source指令中必须包含@type参数，该参数用来指定使用哪个输入插件，比如我们还可以用tail插件来读取文件的内容。 路由source指令把事件提交到Fluentd的路由引擎。一个事件由三个实体组成：tag、time和record。tag是由’.’分割的字符串组成，被内部路由引擎使用。time由input插件指定，必须是Unix时间戳格式。record是一个Json对象。 强烈推荐使用小写字母、数字和下划线来命名tag，虽然其他的字符也是合法的。 “match”: 定义数据的输出目标match指令通过匹配tag字段来将事件输出到其他的系统。同样match指令也必须指定@type参数，该参数用来指定使用哪个输出插件。在下面的例子中，只有myapp.access的tag能够匹配到该输出插件。 1234&lt;match myapp.access&gt; @type file path /var/log/fluent/access&lt;/match&gt; 匹配模式下面的这些匹配模式可以在&lt;match&gt;中使用，用来匹配tag: *用来匹配tag的一部分（比如：a.*可以匹配a.b，但是不能匹配a或者a.b.c） **可以用来匹配tag的0个或多个部分（比如：a.**可以匹配a、a.b和a.b.c） {X,Y,Z}匹配X,Y或者Z（比如：{a,b}可以匹配a和b，但是不能匹配c。他可以和*或者**结合起来一起使用。） 如果有多个匹配模式写在&lt;match&gt;里面，则可以用空格分开(比如：&lt;match a b&gt;能够匹配a和b。&lt;match a.** b.* &gt;能够匹配a,a.b,a.b.c和b.d。) 匹配顺序Fluentd是按顺序匹配的，先在配置文件里面出现的match会先匹配。下面的例子中myapp.access永远都不会被匹配到。 123456789# ** matches all tags. Bad :(&lt;match **&gt; @type blackhole_plugin&lt;/match&gt;&lt;match myapp.access&gt; @type file path /var/log/fluent/access&lt;/match&gt; “filter”：事件处理管道“filter”指令的语法和”match”指令的语法相同，但是”filter”能够在管道中被连起来处理，如下所示： 1Input -&gt; filter 1 -&gt; ... -&gt; filter N -&gt; Output 下面的例子展示了record_transformer fliter的用法。source首先会接收到一个{“event”:”data”}的事件，然后该事件会首先被路由到filter，filter会增加一个host_param的字段到record中，然后再把该事件发送到match中。 1234567891011121314151617# http://this.host:9880/myapp.access?json=&#123;\"event\":\"data\"&#125;&lt;source&gt; @type http port 9880&lt;/source&gt;&lt;filter myapp.access&gt; @type record_transformer &lt;record&gt; host_param \"#&#123;Socket.gethostname&#125;\" &lt;/record&gt;&lt;/filter&gt;&lt;match myapp.access&gt; @type file path /var/log/fluent/access&lt;/match&gt; “system”：设置系统范围配置以下的配置能够由”system”指令指定。也可以通过Fluentd的配置选项设置相同的配置: log_level suppress_repeated_stacktrace emit_error_log_interval suppress_config_dump without_source process_name (只能用”system”指令指定) 下面是一些例子： 123456&lt;system&gt; # 等价于-qq选项 log_level error #等价于--without-source选项 without_source&lt;/system&gt; 123&lt;system&gt; process_name fluentd1&lt;/system&gt; process_name用来指定Fluentd监控进程和工作进程的名字，通过ps可以看到 123% ps aux | grep fluentd1foo 45673 0.4 0.2 2523252 38620 s001 S+ 7:04AM 0:00.44 worker:fluentd1foo 45647 0.0 0.1 2481260 23700 s001 S+ 7:04AM 0:00.40 supervisor:fluentd1 “label”：用来组织filter和match“label”指令用来降低tag路由的复杂度，通过”label”指令可以用来组织filter和match的内部路由。下面是一个配置的例子，由于”label”是内建的插件，所以他的参数需要以@开头。 123456789101112131415161718192021222324252627282930&lt;source&gt; @type forward&lt;/source&gt;&lt;source&gt; @type tail @label @SYSTEM&lt;/source&gt;&lt;filter access.**&gt; @type record_transformer &lt;record&gt; # ... &lt;/record&gt;&lt;/filter&gt;&lt;match **&gt; @type elasticsearch # ...&lt;/match&gt;&lt;label @SYSTEM&gt; &lt;filter var.log.middleware.**&gt; @type grep # ... &lt;/filter&gt; &lt;match **&gt; @type s3 # ... &lt;/match&gt;&lt;/label&gt; 在上面的例子中，forward的数据源的事件被路由到record_transformer filter和elasticsearch output中。tail数据源被路由到@system里面的grep filter和s3 output中。 @ERROR label@ERROR label是内建的label，用来记录emit_error_event错误事件的。如果在配置文件里面设置了，当有相关的错误发生（比如：缓冲区已满或无效记录）的话，该错误事件就会被发送到&lt; label @ERROR &gt;。 “@include”：重用配置可以通过”@include”来导入其他的配置文件，配置文件是按顺序导入的。如果使用模式匹配的话，文件是按字母顺序导入的。 12345678# If you have a.conf,b.conf,...,z.conf and a.conf / z.conf are important...# This is bad@include *.conf# This is good@include a.conf@include config.d/*.conf@include z.conf 如果导入的文件有顺序的要求的话，最好自己主动写导入的语句，模式匹配导入容易出错。 支持的数据类型每个插件都需要一些参数。例如：in_tail插件有rotate_wait和pos_file这两个参数。每个参数都有对应的类型与其关联。下面是这些类型的定义： string 类型：该类型被解析成一个字符串。string类型可以有三种形式：不带引号的字符串、带单引号的字符串和带双引号的字符串。 integer 类型：该类型被解析成一个整数。 float 类型：该类型被解析成一个浮点数。 size 类型：该类型用来解析成有多少个字节。可以在整数后面加上k/K、m/M、g/G、t/T，对应的是计算机学科的度量单位。比如：12k表示为12*1024后的数值。 time 类型：该类型被解析成时间。可以在浮点数后面加上s、m、h和d分别表示为秒、分、小时、天。可以用0.1表示100ms。 array 类型：该类型被解析成JSON数组。这种类型还支持缩写，比如：[“key1”, “key2”]可以缩写成key1,key2。 hash 类型：该类型被解析成JSON对象。这种类型也支持缩写，比如：{“key1”:”value1”, “key2”:”value2”}可以缩写成key1:value1,key2:value2。 常见的插件参数这些参数是系统保留的并且带有@前缀。 @type: 指定插件的类型。 @id: 指定插件的id。 @label：用来指定标签。 @log_level：用来指定每个插件的log级别。 检查配置文件通过–dry-run选项，可以在不启动插件的情况下检查配置文件。 1$ fluentd --dry-run -c fluent.conf 格式建议双引号包起来的字符串、数组和哈希类型支持多行123456789str_param \"foo # This line is converted to \"foo\\nbar\". NL is kept in the parameterbar\"array_param [ \"a\", \"b\"]hash_param &#123; \"k\":\"v\", \"k1\":10&#125; 如果想让[或者{开头的字符串不被解析成数组或者对象，则需要用’或者“把该字符串包起来。 123456789&lt;match **&gt; @type mail subject \"[CRITICAL] foo's alert system\"&lt;/match&gt;&lt;match tag&gt; @type map map '[[\"code.\" + tag, time, &#123; \"code\" =&gt; record[\"code\"].to_i&#125;], [\"time.\" + tag, time, &#123; \"time\" =&gt; record[\"time\"].to_i&#125;]]' multi true&lt;/match&gt; 嵌入Ruby代码可以在”包住的#{}里面执行Ruby代码，这可以用来获取一些机器的信息，比如hostname。 12host_param \"#&#123;hostname&#125;\" # This is same with Socket.gethostname@id \"out_foo#&#123;worker_id&#125;\" # This is same with ENV[\"SERVERENGINE_WORKER_ID\"] 在双引号字符串中，\\是转义字符\\被解释为转义字符。你需要用\\来设置”，\\r，\\n，\\t，\\或双引号字符串中的多个字符。 1str_param \"foo\\nbar\" # \\n is interpreted as actual LF character","categories":[{"name":"日志收集","slug":"日志收集","permalink":"https://lintingbin2009.github.io/categories/日志收集/"}],"tags":[{"name":"Fluentd","slug":"Fluentd","permalink":"https://lintingbin2009.github.io/tags/Fluentd/"}]},{"title":"图片转像素风实现","slug":"图片转像素风实现","date":"2018-03-30T04:13:27.000Z","updated":"2018-05-04T15:15:55.988Z","comments":true,"path":"2018/03/30/图片转像素风实现/","link":"","permalink":"https://lintingbin2009.github.io/2018/03/30/图片转像素风实现/","excerpt":"","text":"Python用来写各种小工具简直是神器，昨天晚上花了点时间实现了一个图片转像素风的小工具，下面附上图片Demo和代码 下面是实现的具体代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from PIL import Image import argparse # 命令行输入参数处理 parser = argparse.ArgumentParser() parser.add_argument('file') # 输入文件 parser.add_argument('-o', '--output') # 输出文件 parser.add_argument('--maxlen', type=int, default=150) # 获取参数 args = parser.parse_args() IMG = args.file MAXLEN = args.maxlen OUTPUT = args.outputdef resize_image(im): (width, height) = im.size max_len = max(width, height) if max_len &lt;= MAXLEN: return im else: scale = max_len / MAXLEN size = (int(width//scale), int(height//scale)) return im.resize(size, Image.NEAREST) if __name__ == '__main__': im = Image.open(IMG) im = resize_image(im) txt = \"\" (width, height) = im.size for i in range(height): for j in range(width): (r, b, g) = im.getpixel((j, i)) txt += \"&lt;font style='background:rgb(&#123;&#125;,&#123;&#125;,&#123;&#125;);display:inline-block;width:3px;height:3px;margin:1px;'&gt;&lt;/font&gt;\".format(r, b, g) txt += '&lt;/br&gt;' if not OUTPUT: OUTPUT = \"output.txt\" with open(OUTPUT,'w') as f: f.write(txt)","categories":[{"name":"Python","slug":"Python","permalink":"https://lintingbin2009.github.io/categories/Python/"}],"tags":[{"name":"图片","slug":"图片","permalink":"https://lintingbin2009.github.io/tags/图片/"},{"name":"小工具","slug":"小工具","permalink":"https://lintingbin2009.github.io/tags/小工具/"}]},{"title":"使用erlang:get_stacktrace注意避开的坑","slug":"使用erlang-get-stacktrace注意避开的坑","date":"2018-03-29T03:03:03.000Z","updated":"2018-05-04T15:15:55.987Z","comments":true,"path":"2018/03/29/使用erlang-get-stacktrace注意避开的坑/","link":"","permalink":"https://lintingbin2009.github.io/2018/03/29/使用erlang-get-stacktrace注意避开的坑/","excerpt":"","text":"之前在使用erlang:get_stacktrace()函数的时候发现不能正确的获取发生异常的栈内容，但是错误类型和原因却是正常，感觉非常奇怪，下面是具体的代码： 12345678910dispatch_cmd(User, Mod, Msg) -&gt; try Mod:req(User, Msg) of Result -&gt; Result catch Class:Reason -&gt; monitor:notify(ws_dispatch_crash, io_lib:format(\"&lt;error-info: ~p:req ~p:~p&gt;\", [Mod, Class, Reason])), ?ERROR(\"Req Msg: ~p.~nStacktrace: ~s\", [?PR(Msg), ?PR_ST(erlang:get_stacktrace(), &#123;Class, Reason&#125;)]), ?ERR_AT_DISPATCH_CMD end. 上面的代码有什么问题呢？ 主要的问题是在调用erlang:get_stacktrace()之前执行了其他有可能会有异常捕获的语句，而在io_lib:format里面会有catch函数，如果io_lib:format函数里面的catch被调用的话，erlang:get_stacktrace()返回的就不是我们想要打印的异常栈，而是io_lib:format里面的异常栈。 如何解决？ 在catch之后里面立马调用erlang:get_stacktrace() 1234567891011dispatch_cmd(User, Mod, Msg) -&gt; try Mod:req(User, Msg) of Result -&gt; Result catch Class:Reason -&gt; Stacktrace = erlang:get_stacktrace(), monitor:notify(ws_dispatch_crash, io_lib:format(\"&lt;error-info: ~p:req ~p:~p&gt;\", [Mod, Class, Reason])), ?ERROR(\"Req Msg: ~p.~nStacktrace: ~s\", [?PR(Msg), ?PR_ST(Stacktrace, &#123;Class, Reason&#125;)]), ?ERR_AT_DISPATCH_CMD end. 据说erlang的开发团队也认为erlang:get_stacktrace()是一个不好的东西，会在OTP 21中把它废弃掉，有一位叫@peterdmv的开发人员是这样说的： 1234567erlang:get_stacktrace/0 is deprecated in OTP 21, you can use the following expression instead:try Exprcatch Class:Reason:Stacktrace -&gt; &#123;Class,Reason,Stacktrace&#125;end 我试了下这个新语法，在OTP 20.3上面还不行，应该在接下来的OTP 21中能够使用它吧~","categories":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/categories/Erlang/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"Git远程推送时记住用户名和密码","slug":"Git远程推送时记住用户名和密码","date":"2018-03-11T09:04:20.000Z","updated":"2018-05-04T15:15:55.981Z","comments":true,"path":"2018/03/11/Git远程推送时记住用户名和密码/","link":"","permalink":"https://lintingbin2009.github.io/2018/03/11/Git远程推送时记住用户名和密码/","excerpt":"当使用HTTPS协议推送代码到Git仓库时，发现每次都需要输入密码，操作起来非常麻烦。下面介绍几种免去输入密码的方法。","text":"当使用HTTPS协议推送代码到Git仓库时，发现每次都需要输入密码，操作起来非常麻烦。下面介绍几种免去输入密码的方法。 HTTPS协议推送使用HTTPS协议，有一种简单粗暴的方式是在远程地址中带上密码。 1&gt; git remote set-url origin http://yourname:password@bitbucket.org/yourname/project.git 还有一种方法，是创建文件存储Git用户名和密码。 以Windows环境为例，在%USERPROFILE%目录中（一般为C:\\Users\\yourname)，打开Git Bash命令行，创建文件 1&gt; touch .git-credentials 在文件中输入仓库域名，这里使用了bitbucket.org。 1https://yourname:password@bitbucket.org 在CMD终端中设置在全局Git环境中，长期存储密码 1&gt; git config --global credential.helper store 其他设置密码方式 记住密码（默认15分钟）：git config --global credential.helper cache 自定义存储时间：git config credential.helper &#39;cache --timeout=3600&#39; SSH协议推送如果原来的推送地址协议是HTTPS，可以通过换成SSH协议，在远程仓库添加SSH Key来实现推送时免账户密码输入。 123&gt; git remote -v // 查看远程地址&gt; git remote rm origin // 删除原有的推送地址&gt; git remote add origin git@github.com:&lt;用户名&gt;/版本库名 或者 12&gt; git remote -v&gt; git remote set-url origin git@github.com:&lt;用户名&gt;/版本库名 执行推送。 1&gt; git push -u origin master 发现提示权限不够。 12345678910The authenticity of host &apos;bitbucket.org (104.192.143.1)&apos; can&apos;t be established.RSA key fingerprint is SHA256:zzXQOXSRBEiUtuE8AikJYKwbHaxvSc0ojez9YXaGp1A.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;bitbucket.org,104.192.143.1&apos; (RSA) to the list of known hosts.Permission denied (publickey).fatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists. 需要在本地创建该帐号的RSA Key。可以参考以下两篇文章： Windows下配置SSH连接Github Git如何在本地生成多个SSH key 然后再执行推送。 1&gt; git push -u origin master 就可以推送成功了。","categories":[{"name":"教程","slug":"教程","permalink":"https://lintingbin2009.github.io/categories/教程/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://lintingbin2009.github.io/tags/Git/"}]},{"title":"git如何在本地生成多个ssh key","slug":"Git如何在本地生成多个SSH-key","date":"2018-03-11T09:03:15.000Z","updated":"2018-05-20T06:58:55.762Z","comments":true,"path":"2018/03/11/Git如何在本地生成多个SSH-key/","link":"","permalink":"https://lintingbin2009.github.io/2018/03/11/Git如何在本地生成多个SSH-key/","excerpt":"在本地上传Git项目到远程时，本地需要对应的Git账户信息和允许连接的SSH key信息，远程才会允许上传。","text":"在本地上传Git项目到远程时，本地需要对应的Git账户信息和允许连接的SSH key信息，远程才会允许上传。 Windows下配置SSH连接Github介绍了如何生成SSH key。但当遇到需要有多个Git账户信息时怎么办呢，如果删除密钥重新创建，那每次切换账户时都要重复这样的操作，太过繁琐。 这个问题我们可以通过在~/.ssh目录下增加config文件来解决。 生成SSH key生成SSH key，并指定文件名，避免覆盖原有的默认id_rsa文件。 1$ ssh-keygen -t rsa -f ~/.ssh/id_rsa.another -C &lt;Git注册邮箱&gt; windows用户打开Git Bash来执行ssh-keygen命令。 配置config文件在~/.ssh下添加config文件，如果已经存在，就直接打开修改。 1$ touch ~/.ssh/config // 创建 在config文件中添加如下信息。其中Host后面添加远程Git仓库域名，IdentityFile填写对应的id_rsa文件，User添加Git用户名。 123Host github.com IdentityFile ~/.ssh/id_rsa.another User anotherUser 上传SSH key在远程Git账号中添加SSH key，将id_rsa.another.pub中的内容全部粘贴进去。 pub信息一般以ssh-rsa开头 测试SSH key1ssh -T git@git.com // 或者其他域名地址 弹出成功信息，则表示SSH key添加成功，接下来就可以推送代码到远程了。","categories":[{"name":"教程","slug":"教程","permalink":"https://lintingbin2009.github.io/categories/教程/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://lintingbin2009.github.io/tags/Git/"}]},{"title":"Windows下配置SSH连接Github","slug":"Windows下配置SSH连接Github","date":"2018-03-11T08:55:55.000Z","updated":"2018-05-04T15:15:55.982Z","comments":true,"path":"2018/03/11/Windows下配置SSH连接Github/","link":"","permalink":"https://lintingbin2009.github.io/2018/03/11/Windows下配置SSH连接Github/","excerpt":"当使用git协议推送本地代码到远程时，需要配置ssh连接到GitHub。","text":"当使用git协议推送本地代码到远程时，需要配置ssh连接到GitHub。 设置账户打开系统cmd。设置Git的user name和email。 12&gt; git config --global user.name &quot;username&quot;&gt; git config --global user.email &quot;username@domain.com&quot; 生成ssh key 查看ssh密钥 查看是否已经有了ssh密钥。 12&gt; C:&gt; cd %USERPROFILE%/.ssh 默认.ssh文件夹会在用户文件夹中生成。可以前往该路径查看。如果没有该文件夹，终端会提示系统找不到指定的路径,有的话可以备份删除。 %USERPROFILE%是环境变量，表示当前用户文件夹路径。 因为%USERPROFILE%一般在C盘，从其他盘无法直接cd到该路径，所以需要先切换到c盘。 生成密钥 1&gt; ssh-keygen -t rsa -C &quot;username@domain.com&quot; 或者 1ssh-keygen -t rsa -C &quot;username@domain.com&quot; -f %USERPROFILE%/.ssh/githug_blog_keys #生成ssh key的名称为githug_blog_keys，慎用容易出现其它异常。 产生如下交互提示: 123456789101112131415161718192021Generating public/private rsa key pair.Enter file in which to save the key (/c/Users/username/.ssh/id_rsa):Created directory &apos;/c/Users/username/.ssh&apos;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /c/Users/username/.ssh/id_rsa.Your public key has been saved in /c/Users/username/.ssh/id_rsa.pub.The key fingerprint is:SHA256:ggjGi2UqwYTJR9TP6fdpqsY+COeMdeTPL+IW0mo4eUI username@domain.comThe key&apos;s randomart image is:+---[RSA 2048]----+|oooo. ||*. . . ||o++ o . ||o*.. . = ||+.. . * S ||. E + B . || . X * = . . || * B * + + || = =++.=. |+----[SHA256]-----+ 命令生成两个文件，默认名称是id_rsa和id_rsa.pub。 复制id_rsa.pub中的内容到远程GitHub账号中就行了。 默认下，cmd无法识别ssh-keygen命令。可以打开git bash 执行上述命令。 如何找到http://github.com上的ssh设置：右上角图标（view profile and more）=》Settings =》 SSH and GPG keys，在Key输入框中输入公钥。公钥内容形如： 1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD2cauWf8lNjbED9RvLBWoEXB3Lq5NRLWqVYSaFcTmZ1Qsak2LdR/9bkdTVMsTyqVxnB+bDMlVJlvPP6Zv5dDwEdcdytALUGCSdOXAmRtgxHZPEnKl8Hyl5wZdSNi0mwXYbjpUZ7HEw3vU8K/5whVVCbIzkqnlLAs9nXdORNkidOZRnyt+ETQzU/F1KVUb9HoMbB1Cw0zLvWTRIOHRXa2mKjNHS7W9HJiPEWQaeEXwh1CoredCxs0K7+KBfMkdgNYnDudtz2/AmH7qHnaVsdYNkg1V4XvWJ8Yn7Pkw1SpvTkaXXCiyA5wsPGgLFWSB+dgNroGwqw4X96/ZUfFHDtD/t yanyinhong@baidu.com","categories":[{"name":"教程","slug":"教程","permalink":"https://lintingbin2009.github.io/categories/教程/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://lintingbin2009.github.io/tags/Git/"}]},{"title":"关于erlang-mysql-driver timeout的bug分析","slug":"timeout的bug分析","date":"2018-03-10T14:08:17.000Z","updated":"2018-05-04T15:15:55.986Z","comments":true,"path":"2018/03/10/timeout的bug分析/","link":"","permalink":"https://lintingbin2009.github.io/2018/03/10/timeout的bug分析/","excerpt":"","text":"我们游戏之前使用的是erlang-mysql-driver来连接数据库，经常会碰到一些timeout和一条纪录被重复插入多次的bug，后面把erlang-mysql-driver替换成emysql就没有问题了。研究了下erlang-mysql-driver的源代码才知道具体的问题出在哪里，下面就简单的介绍下这个问题，同时介绍下emysql是如何避免这个问题的 erlang-mysql-driver 用法123mysql:start_link(DB, Pool, Server, Port, User, Passwd, DBName, fun mysql_log/4, utf8),[mysql:connect(DB, Pool, Server, Port, User, Passwd, DBName, utf8, true, true) || _ &lt;- lists:seq(1, PoolCount)] 使用erlang-mysql-driver的时候首先要用mysql:start_link来建立一个连接池，然后再自己调用mysql:connect来建立多个连接。对应的erlang-mysql-driver库里面会创建一个名为DB的连接池gen_server，然后再建立多个mysql数据库的连接，每个连接会有一个进程来接管，并把这些进程和连接信息放入连接池gen_server中。按道理我们建立了多个数据库的连接，在进行数据库操作的时候应该能够并发访问数据库的，确实erlang-mysql-driver实现也是多进程访问数据库的，但是由于连接池gen_server的单点瓶颈，会导致一些事实上成功的操作被认为是失败的。 erlang-mysql-driver 的sql执行过程在我们执行一个mysql:execute的函数的时候，具体的执行过程如下： 123456789101112131415161718192021222324252627%% mysql.erl 文件中的代码execute(SvrName, PoolId, Name, Params, Timeout) -&gt; case get(?STATE_VAR) of undefined -&gt; call_server(SvrName, &#123;execute, SvrName, PoolId, Name, Params&#125;, Timeout); State -&gt; case mysql_conn:execute_local(SvrName, State, Name, Params) of &#123;ok, Res, NewState&#125; -&gt; put(?STATE_VAR, NewState), Res; Err -&gt; Err end end. handle_call(&#123;execute, SvrName, PoolId, Name, Params&#125;, From, State) -&gt; with_next_conn(PoolId, State, fun(Conn, State1) -&gt; case gb_trees:lookup(Name, State1#state.prepares) of none -&gt; &#123;reply, &#123;error, &#123;no_such_statement, Name&#125;&#125;, State1&#125;; &#123;value, &#123;_Stmt, Version&#125;&#125; -&gt; mysql_conn:execute(SvrName, Conn#conn.pid, Name, Version, Params, From), &#123;noreply, State1&#125; end end); 1234567891011121314151617181920212223%% mysql_conn的代码execute(SvrName, Pid, Name, Version, Params, From, Timeout) -&gt; send_msg(Pid, &#123;execute, SvrName, Name, Version, Params, From&#125;, From, Timeout). loop(State) -&gt; RecvPid = State#state.recv_pid, LogFun = State#state.log_fun, receive .... &#123;execute, SvrName, Name, Version, Params, From&#125; -&gt; State1 = case do_execute(State, SvrName, Name, Params, Version) of &#123;error, _&#125; = Err -&gt; send_reply(From, Err), State; &#123;ok, Result, NewState&#125; -&gt; send_reply(From, Result), NewState end, loop(State1); send_reply(GenSrvFrom, Res) -&gt; gen_server:reply(GenSrvFrom, Res). mysql:execute在执行的过程中首先调用名为DB的连接池gen_server，该gen_server会执行with_next_conn选择一个持有数据库连接的进程（进程x），然后通过mysql_conn中的send_msg函数向进程x发送需要执行的sql语句，sql语句发送成功后gen_server会返回noreply，这时调用mysql:execute的进程会一直阻塞，直到进程x执行gen_server:reply来返回结果。 通过上面的执行过程我们可以知道以下两点： mysql:execute的timeout为gen_server:call调用的timeout时间 连接池gen_server只要把sql语句发送给进程x，进程x就会去执行（可能执行的比较慢，但是已经加入进程x的信箱） 现在我们可以知道mysql:execute返回timeout的情况有两种： 连接池gen_server太过繁忙，mysql:execute的请求还没执行，mysql:execute就已经timeout 连接池gen_server已经成功执行请求，返回noreply，mysql:execute的执行进程一直在等待进程x的返回，而进程x一直不返回，这时mysql:execute触发timeout 不管是上述那种timeout情况，只要是mysql数据库没有问题，sql语句都能够执行成功（可能会执行的慢点）。mysql:execute的调用者在发现mysql:execute返回timeout的情况下，肯定会认为sql语句没有执行成功，这时候会重新调用mysql:execute，导致一条相同的记录被多次插入。 emysql 的sql执行过程emysql的连接池也会有一个gen_server进行管理，emysql:execute在执行的过程中是去向该gen_server申请一个可用的连接，然后再spawn一个进程来执行sql语句，而不是委托该gen_server来执行sql语句，从而避免了这个timeout的bug。 总结通过上面的分析，我觉得erlang-mysql-driver会写出这个bug的原因主要是对gen_server noreply的误用。所以以后如果有需要用noreply的话，要注意避免该问题。","categories":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/categories/Erlang/"}],"tags":[{"name":"erlang-mysql-driver","slug":"erlang-mysql-driver","permalink":"https://lintingbin2009.github.io/tags/erlang-mysql-driver/"},{"name":"emysql","slug":"emysql","permalink":"https://lintingbin2009.github.io/tags/emysql/"}]},{"title":"efficiency-guide:进程","slug":"efficiency-guide-进程","date":"2017-10-15T06:38:57.000Z","updated":"2018-05-04T15:15:55.984Z","comments":true,"path":"2017/10/15/efficiency-guide-进程/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/15/efficiency-guide-进程/","excerpt":"","text":"这篇文章主要介绍进程的堆大小的优化，以及模块内的常量池，如果之前没有看过的，值得一看。 创建一个进程与操作系统中的线程和进程相比，Erlang进程是轻量级的。 新创建的Erlang进程在非SMP和非HiPE模式下中使用309个字的内存。（SMP和HiPE支持会增加一些内存）进程占用的内存大小可以通过以下方式得到： Erlang (BEAM) emulator version 5.6 [async-threads:0] [kernel-poll:false] Eshell V5.6 (abort with ^G) 1&gt; Fun = fun() -&gt; receive after infinity -&gt; ok end end. #Fun&lt;…&gt; 2&gt; {_,Bytes} = process_info(spawn(Fun), memory). {memory,1232} 3&gt; Bytes div erlang:system_info(wordsize). 309 此大小包括堆区域（包括堆栈）的233个字。垃圾收集器根据需要增加堆。 进程的主（外）循环必须是尾递归的。否则，堆栈会一直增长直到进程终止。 123456789101112%% 不要这样做loop() -&gt; receive &#123;sys, Msg&#125; -&gt; handle_sys_msg(Msg), loop(); &#123;From, Msg&#125; -&gt; Reply = handle_msg(Msg), From ! Reply, loop() end, io:format(\"Message is processed~n\", []). 对io:format/2的调用永远不会被执行，但是每次loop/0被递归调用时，返回地址仍然被推送到堆栈。该函数的正确尾递归版本如下所示： 12345678910loop() -&gt; receive &#123;sys, Msg&#125; -&gt; handle_sys_msg(Msg), loop(); &#123;From, Msg&#125; -&gt; Reply = handle_msg(Msg), From ! Reply, loop() end. 初始堆大小对于具有数十万甚至数百万个进程的Erlang系统，默认的初始堆大小为233个字节是相当保守的。垃圾收集器根据需要增加和收缩堆。 在使用相对较少进程的系统中，可以通过使用erl的+h选项或使用spawn_opt/4的min_heap_size选项在每个进程的基础上增加最小堆大小来提高性能。主要有以下两个好处： 虽然垃圾收集器可以逐步增长堆的大小，但这比生成进程时直接建立更大的堆来的低效。 垃圾收集器还可以收缩堆，如果它比存储在其上的数据量大得多;设置最小堆大小可以防止这种情况发生。 注意：由于虚拟机可以使用更多的内存，则内存回收的次数将减少，因此Binary可以被保存的更久才进行内存回收 在具有许多进程的系统中，运行时间很短的计算任务可以生成具有更大最小堆大小的新进程。当进程完成后，它将计算结果发送到另一个进程并终止。如果正确计算最小堆大小，则该过程可能根本不需要执行任何垃圾回收。如果没有进行适当的测量，则不进行此优化。 进程消息除了在同一个Erlang节点上的refc binary，Erlang进程之间的所有消息都会被复制。 当消息发送到另一个Erlang节点上的进程时，它首先被编码为Erlang外部格式，然后通过TCP/IP套接字发送。接收的Erlang节点解码消息并将其分发到相应的进程。 常量池（Constant Pool）Erlang常量（也称为文字(literals)）保存在常量池中;每个加载的模块都有自己的池。以下函数不会在每次调用时构建元组（仅在下次运行垃圾回收时丢弃它），因为元组位于模块的常量池中： 12days_in_month(M) -&gt; element(M, &#123;31,28,31,30,31,30,31,31,30,31,30,31&#125;). 但是如果一个常量被发送到另一个进程（或存储在一个Ets表中），那么它将被复制。原因是运行时系统必须能够跟踪所有对常量的引用，以正确卸载包含常量的代码。（当代码被卸载时，这些常量被复制到引用它们的进程的堆中。）复制常量可能在将来的Erlang/OTP版本中被删除。 共享丢失在以下情况下，共享不会保留： 当一个共享变量被发送到另一个进程 当一个共享变量作为spawn调用中的初始进程参数传递时 当共享变量被存储在Ets表中时 这是一个优化。大多数应用不发送带有共享变量的消息。 以下示例显示如何创建共享变量： 1234567kilo_byte() -&gt; kilo_byte(10, [42]).kilo_byte(0, Acc) -&gt; Acc;kilo_byte(N, Acc) -&gt; kilo_byte(N-1, [Acc|Acc]). kilo_byte/1创建一个嵌套列表。如果调用list_to_binary/1，则可以将嵌套列表转换为1024字节的Binary： 1&gt; byte_size(list_to_binary(efficiency_guide:kilo_byte())). 1024 使用erts_debug:size/1 BIF，可以看出嵌套列表只需要22个字的堆空间： 2&gt; erts_debug:size(efficiency_guide:kilo_byte()). 22 使用erts_debug:flat_size/1 BIF，可以忽略共享来计算嵌套列表的大小。当它被发送到另一个进程或存储在Ets表中时，它将成为列表的实际大小： 3&gt; erts_debug:flat_size(efficiency_guide:kilo_byte()). 4094 将数据插入到Ets表中，则可以确认共享丢失： 4&gt; T = ets:new(tab, []). #Ref&lt;0.1662103692.2407923716.214181&gt; 5&gt; ets:insert(T, {key,efficiency_guide:kilo_byte()}). true 6&gt; erts_debug:size(element(2, hd(ets:lookup(T, key)))). 4094 7&gt; erts_debug:flat_size(element(2, hd(ets:lookup(T, key)))). 4094 当数据被插入到Ets表时，erts_debug:size/1和erts_debug:flat_size/1返回相同的值。共享已经丢失。 在未来的Erlang/OTP版本中，可能会实现一种方式（可选）保留共享。 SMPSMP模式（在R11B中引入）通过运行多个Erlang调度器线程（通常与内核数相同）来利用多核计算机的性能。每个调度器线程以与非SMP模式中的Erlang调度器相同的方式调度Erlang进程。 为了通过使用SMP模式获得更多的性能提升，应用程序大多数时候都必须有多个可运行的Erlang进程。否则，Erlang虚拟机仍然只能运行一个Erlang进程，但是仍然必须增加锁的开销。尽管Erlang/OTP尝试尽可能减少锁开销，但它永远不会变为零。","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"efficiency-guide:表和数据库","slug":"efficiency-guide-表和数据库","date":"2017-10-15T05:36:22.000Z","updated":"2018-05-04T15:15:55.983Z","comments":true,"path":"2017/10/15/efficiency-guide-表和数据库/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/15/efficiency-guide-表和数据库/","excerpt":"","text":"这篇文件主要讲一些简单的Erlang数据库注意要点，干货好像没那么多，可以快速浏览下。 Ets、Dets、Mnesia每个Ets的例子都有一个与之对应的Mnesia的例子。一般来说，所有Ets示例也适用于Dets表。 Select/Match操作在Ets和Mnesia表上的Select/Match操作是非常低效的。他们通常需要扫描整个表。需要优化数据的结构来减少对Select/Match操作的使用。但是，如果确实需要Select/Match操作，它仍然比使用tab2list更有效率。函数ets:select/2和mnesia:select/3是优于ets:match/2、ets:match_object/2和mnesia:match_object/3的。 在某些情况下，Select/Match操作是不需要扫描整张表的。例如，在ordered_set的表中搜索，或者搜索的Mnesia表的字段是有建立索引的。 当创建要在Select/Match操作中使用的记录时，如果希望大多数字段具有值“_”。最简单和最快捷的方法如下： 1#person&#123;age = 42, _ = '_'&#125;. 删除一个元素如果元素不存在于表中，则删除操作被认为是成功的。因此，在删除之前，所有尝试检查元素是否存在于Ets / Mnesia表中都是不必要的。以下是Ets表的示例： 12345678910111213%% 直接删就可以了...ets:delete(Tab, Key),...%% 这样做，效率低而且没有意义...case ets:lookup(Tab, Key) of [] -&gt; ok; [_|_] -&gt; ets:delete(Tab, Key)end,... 非持久数据库存储对于非持久数据库存储，优先考虑Ets表，而不是Mnesia local_content表。与Ets写入相比，即使是Mnesia dirty_write操作具有常量的开销。 Mnesia也必须检查表是否被复制或具有索引，这涉及每个dirty_write至少一次Ets查找。因此，Ets总是比Mnesia写的快。 tab2list简单的说tab2list肯定不要用，除非是需要返回所有的Ets数据。如果需要选择一部分的数据可以用Select/Match操作，因为获取Ets的数据是需要拷贝的，tab2list返回所有的数据，需要大量的拷贝，效率非常低。 ordered_set表ordered_set仅保证按Key的顺序处理对象。即使Key不包含在结果中，也可以按Key顺序显示ets:select/2等函数的结果（还有select,match_object,foldl,first,next等函数）。 优化掉Select/Match操作Ets由于Ets用Key来Lookup获取数据是可以在常量的时间内完成的（使用哈希和树结构），所以如果要优化掉Select操作，可以再建一个要Select的字段到先前的Ets的Key的新的Ets表就可以了，这样通过两次lookup就可以取得想要的数据了。 Mnesia在Mnesia中只要多建立一些想要Select字段的索引就可以，这样就不用扫描整张表了。","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"efficiency-guide:函数","slug":"efficiency-guide-函数","date":"2017-10-15T04:15:32.000Z","updated":"2018-05-04T15:15:55.983Z","comments":true,"path":"2017/10/15/efficiency-guide-函数/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/15/efficiency-guide-函数/","excerpt":"","text":"本篇文章主要介绍函数的模式匹配和调用，函数的模式匹配优化还是有必要了解一下的。 模式匹配在函数的头部以及case和receive子句中的模式匹配都会被编译器优化。但是有一些例外，重新排列匹配子句没有什么好处。 Binary的模式匹配就是一个例外。编译器不重新排列与Binary相匹配的子句。最后放置与空Binary匹配的子句通常比放置在第一个的子句执行的更快。 下面是另一种例外的例子： 1234567atom_map1(one) -&gt; 1;atom_map1(two) -&gt; 2;atom_map1(three) -&gt; 3;atom_map1(Int) when is_integer(Int) -&gt; Int;atom_map1(four) -&gt; 4;atom_map1(five) -&gt; 5;atom_map1(six) -&gt; 6. 问题在于带有变量Int的子句。由于变量可以匹配任何内容，包括原子four、five、six，后续的子句也匹配，所以编译器必须生成次优代码，执行如下： 首先，将输入值与one、two、three（使用二分查找，即使有很多值也非常有效）来选择要执行的前三个子句中的哪一个（如果有的话）。 如果前三个子句中没有一个匹配，则第四个子句匹配变量的话会始终匹配。 如果测试is_integer(Int)成功，则执行第四个子句。 如果测试失败，则将输入值与four、five、six进行比较，并选择适当的子句。 （如果没有匹配成功，则会产生一个function_clause异常。） 如果想让匹配代码更加高效，则上面的代码可以重写成： 1234567atom_map2(one) -&gt; 1;atom_map2(two) -&gt; 2;atom_map2(three) -&gt; 3;atom_map2(four) -&gt; 4;atom_map2(five) -&gt; 5;atom_map2(six) -&gt; 6;atom_map2(Int) when is_integer(Int) -&gt; Int. 或者: 1234567atom_map3(Int) when is_integer(Int) -&gt; Int;atom_map3(one) -&gt; 1;atom_map3(two) -&gt; 2;atom_map3(three) -&gt; 3;atom_map3(four) -&gt; 4;atom_map3(five) -&gt; 5;atom_map3(six) -&gt; 6. 还有下面的例子： 123456map_pairs1(_Map, [], Ys) -&gt; Ys;map_pairs1(_Map, Xs, [] ) -&gt; Xs;map_pairs1(Map, [X|Xs], [Y|Ys]) -&gt; [Map(X, Y)|map_pairs1(Map, Xs, Ys)]. 第一个参数没有问题。它是在所有匹配子句都有的一个变量。有问题的是第二个参数在第二个匹配子句的变量Xs。因为Xs变量可以匹配任何东西，所以编译器不能重新排列匹配子句，而是必须按照上述代码的顺序生成与它们匹配的代码。 如果函数按如下方式重写，编译器就可以自由地重新排列子句： 123456map_pairs2(_Map, [], Ys) -&gt; Ys;map_pairs2(_Map, [_|_]=Xs, [] ) -&gt; Xs;map_pairs2(Map, [X|Xs], [Y|Ys]) -&gt; [Map(X, Y)|map_pairs2(Map, Xs, Ys)]. 编译器将生成与此类似的代码： 123456789101112explicit_map_pairs(Map, Xs0, Ys0) -&gt; case Xs0 of [X|Xs] -&gt; case Ys0 of [Y|Ys] -&gt; [Map(X, Y)|explicit_map_pairs(Map, Xs, Ys)]; [] -&gt; Xs0 end; [] -&gt; Ys0 end. 这可能是最常见的情况，输入列表不是空或非常短。（另一个优点是Dialyzer可以为Xs变量推导出更准确的类型。） 函数调用以下是对不同函数调用类型的代价的粗略的估计。它是在Solaris/Sparc上运行测试出来的基准数据： 调用本地或外部函数（foo()，m:foo()）是最快的。 调用或者apply调用一个匿名函数（Fun(),apply(Fun, [])）的大概时间花费差不多是调用一个本地函数的三倍。 Apply调用一个被导出的函数(Mod:Name(),apply(Mod, Name, [])),大概的时间花费差不多是调用匿名函数的两倍，也就是本地函数调用的六倍 注释和实现细节调用和apply一个匿名函数不涉及任何的哈希表查找。一个匿名函数变量包含一个（间接的）指向匿名函数实现的函数的指针。 apply/3必须在哈希表中查找执行的函数的代码。因此，总是比直接调用或匿名函数调用来的慢。 它不再重要（从性能的角度来看）是否写成： 1Module:Function(Arg1, Arg2) 或者： 1apply(Module, Function, [Arg1,Arg2]) 编译器会将后一种代码重写为前一种的方式。 以下代码会稍微慢一点，因为参数的个数在编译时是未知的。 1apply(Module, Function, Arguments) 递归的内存使用当编写递归函数时，最好使用尾递归的方式，以便它们可以在恒定的内存空间中执行： 最好这样写： 12345678list_length(List) -&gt; list_length(List, 0).list_length([], AccLen) -&gt; AccLen; % Base caselist_length([_|Tail], AccLen) -&gt; list_length(Tail, AccLen + 1). % Tail-recursive 而不要这样写： 1234list_length([]) -&gt; 0. % Base caselist_length([_ | Tail]) -&gt; list_length(Tail) + 1. % Not tail-recursive","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"efficiency-guide:List处理","slug":"efficiency-guide-List处理","date":"2017-10-14T09:21:11.000Z","updated":"2018-05-04T15:15:55.982Z","comments":true,"path":"2017/10/14/efficiency-guide-List处理/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/14/efficiency-guide-List处理/","excerpt":"","text":"关于列表的处理，大多懂得Erlang的同学都差不多知道列表要怎么用，但是关于列表嵌套和拉伸的优化点还是值得一看的。 列表创建不能使用如下代码创建列表，因为每次迭代都会创建一个新的列表: 1234567bad_fib(N) -&gt; bad_fib(N, 0, 1, []).bad_fib(0, _Current, _Next, Fibs) -&gt; Fibs;bad_fib(N, Current, Next, Fibs) -&gt; bad_fib(N - 1, Next, Current + Next, Fibs ++ [Current]). 应该使用如下的代码创建列表: 1234567tail_recursive_fib(N) -&gt; tail_recursive_fib(N, 0, 1, []).tail_recursive_fib(0, _Current, _Next, Fibs) -&gt; lists:reverse(Fibs);tail_recursive_fib(N, Current, Next, Fibs) -&gt; tail_recursive_fib(N - 1, Next, Current + Next, [Current|Fibs]). 列表推导列表推导现在仍然被认为是缓慢的。他们过去常常使用funs来实现，而funs过去很慢。 以下的列表推导： 1[Expr(E) || E &lt;- List] 会被转换成本地的函数实现： 123'lc^0'([E|Tail], Expr) -&gt; [Expr(E)|'lc^0'(Tail, Expr)];'lc^0'([], _Expr) -&gt; []. 如果列表推导的结果不会被使用，则不会构造列表。如下的代码： 1234567891011[io:put_chars(E) || E &lt;- List],ok.%% 或者...case Var of ... -&gt; [io:put_chars(E) || E &lt;- List]; ... -&gt;end,some_function(...),... 上述的代码不会构造列表，所以转换成以下的本地函数实现： 1234'lc^0'([E|Tail], Expr) -&gt; Expr(E), 'lc^0'(Tail, Expr);'lc^0'([], _Expr) -&gt; []. 编译器知道分配给’_’意味着该值不会被使用。因此，以下示例中的代码也将进行优化： 12_ = [io:put_chars(E) || E &lt;- List],ok. 嵌套和拉伸列表(Deep and Flat Lists)lists:flatten/1比++操作更加的低效，在下述的情况中，可以很简单的避免使用lists:flatten/1: 向端口发送数据时。端口了解嵌套列表，所以没有理由在将列表发送到端口之前拉伸列表。 当调用接受嵌套列表的BIF时，例如list_to_binary/1或iolist_to_binary/1。 当知道列表只有一级嵌套时，可以使用list:append/1。 端口例子1234567...port_command(Port, DeepList) %% DO......port_command(Port, lists:flatten(DeepList)) %% DO NOT... 通常会这样向端口发送一个以0为结尾的字符串： 1234...TerminatedStr = String ++ [0], % String=\"foo\" =&gt; [$f, $o, $o, 0]port_command(Port, TerminatedStr)... 上述效率比较低，应该用下述方式代替： 1234...TerminatedStr = [String, 0], % String=\"foo\" =&gt; [[$f, $o, $o], 0]port_command(Port, TerminatedStr) ... Append例子123lists:append([[1], [2], [3]]). %% DOlists:flatten([[1], [2], [3]]). %% DO NOT 递归列表函数普通递归列表函数和尾部递归函数在结束的时候反转列表之间通常没有太大差异。因此，专注于编写好看的代码，并忘记了列表功能的性能。在代码的性能关键部分（仅在那里），用比较高效的写法就行了。 这部分是关于构造列表的列表函数。不构造列表的尾递归函数运行在常量空间中，而相应的普通递归函数使用与列表长度成比例的堆栈空间。 例如，一个将整数列表相加的函数不能写成如下： 12recursive_sum([H|T]) -&gt; H+recursive_sum(T);recursive_sum([]) -&gt; 0. 应该写成: 1234sum(L) -&gt; sum(L, 0).sum([H|T], Sum) -&gt; sum(T, Sum + H);sum([], Sum) -&gt; Sum.","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"efficiency-guide:Binary的构建和匹配","slug":"efficiency-guide-Binary的构建和匹配","date":"2017-10-14T05:11:44.000Z","updated":"2018-05-04T15:15:55.982Z","comments":true,"path":"2017/10/14/efficiency-guide-Binary的构建和匹配/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/14/efficiency-guide-Binary的构建和匹配/","excerpt":"","text":"这篇文章非常详细的介绍了Binary是怎么样构造和匹配的，同时介绍了一些优化的技巧，没看过的一定要仔细看下。 以下代码可以高效的构建Binary(有点奇怪，和List不一样，下面构造Binary的段落会有解释): 1234567my_list_to_binary(List) -&gt; my_list_to_binary(List, &lt;&lt;&gt;&gt;).my_list_to_binary([H|T], Acc) -&gt; my_list_to_binary(T, &lt;&lt;Acc/binary,H&gt;&gt;);my_list_to_binary([], Acc) -&gt; Acc. 以下代码可以高效的匹配Binary: 123my_binary_to_list(&lt;&lt;H,T/binary&gt;&gt;) -&gt; [H|my_binary_to_list(T)];my_binary_to_list(&lt;&lt;&gt;&gt;) -&gt; []. Binary类型是怎么实现的?Binary和Bitstring在虚拟机的内部实现是一样的。在虚拟机的源代码中都叫做Binary。Binary类型在虚拟机内部由四种Binary对象实现: Refc Binaries Refc Binaries由两部分组成： 存储在进程堆上的对象，称为ProcBin Binary对象本身,它存储在所有进程堆之外 Binary对象可以由任意数量的进程引用任意数量的ProcBin。该对象包含一个引用计数器，用于跟踪引用数量，以便在最后一个引用消失时可以将其删除。进程中的所有ProcBin对象都是链表的一部分，因此当ProcBin消失时，垃圾回收器可以跟踪它们并减少二进制中的引用计数器。当构建的Binary大于64Byte的时候就会使用这种类型，此时进程之间发送Binary只是发送一个ProcBin。 Heap Binaries Heap binaries是小型Binary，最多64Byte，并直接存储在进程堆中。当Heap Binary被进程垃圾回收或者是作为消息发送时，都需要被复制。垃圾收集器不需要特殊处理。 Sub Binaries Sub Binaries和match contexts对象能引用refc binary和heap binary对象的部分内容。 Sub Binary是由split_binary/2创建的。一个Sub Binary引用另一个Binary的部分内容（只能引用Refc和Heap Binary，不能引用另一个Sub Binary）。因此，匹配一个Binary类型是比较高效的，因为实际的Binary数据是不会被复制的。 Match Context Match context和Sub binary比较类似，但是Match context专门为Binary匹配优化。（原文比较拗口，这里不做解释了，我也没太看懂） 构建BinaryBinary和Bitstring的append操作是被运行时系统特别优化的，只有在极少数的情况下优化是不起作用的。 如下代码可以解释优化是如何起作用的: 123456Bin0 = &lt;&lt;0&gt;&gt;,Bin1 = &lt;&lt;Bin0/binary,1,2,3&gt;&gt;,Bin2 = &lt;&lt;Bin1/binary,4,5,6&gt;&gt;,Bin3 = &lt;&lt;Bin2/binary,7,8,9&gt;&gt;,Bin4 = &lt;&lt;Bin1/binary,17&gt;&gt;,&#123;Bin4,Bin3&#125; 第1行分配一个Heap Binary给Bin0变量 第2行是append操作。由于Bin0没有涉及append操作，所以创建一个新的Refc Binary，并将Bin0的内容复制到其中。Refc Binary的ProcBin部分存有Binary对象的数据大小，而Binary对象还分配了额外的空间。Binary对象的大小是Bin1或256的大小的两倍，以较大者为准。在这个例子中是256。 第3行更有意思。Bin1已被用于append操作，最后有252字节的未使用内存，因此3个新的字节会被存储在这些空闲的内存中。 第4行。和第3行一样。剩下249个字节，所以存储另外3个新字节没有问题。 第5行。有趣的事情发生。请注意，Bin4是用Bin1来append值17。Bin4将被赋值为&lt;&lt;0,1,2,3,17&gt;&gt;。Bin3将保留其价值&lt;&lt;0,1,2,3,4,5,6,7,8,9&gt;&gt;。显然，运行时系统不能将字节17写入上述的Refc Binary中，因为这会将Bin3的值更改为&lt;&lt;0,1,2,3,4,17,6,7,8,9&gt;&gt;。 运行时系统知道Bin1是先前append操作的结果，所以它将Bin1的内容复制到一个新的Binary，预留额外的存储空间等等类似上面的操作。（这里没有解释为什么运行时系统知道不能写入到Bin1中，如果有兴趣的话可以阅读erl_bits.c源代码） 强制拷贝的情况Binary的append操作优化要求对于Binary:只有一个ProcBin指向一个Refc Binary的Binary对象。原因是优化需要在append操作期间移动（重新分配）Binary对象，并且同时更新ProcBin中的指针。如果有多个ProcBin指向Binary对象，则无法找到并更新它们。 因此，对Binary的某些操作会被做标记，以便在将来做append操作的时候知道是否要强制拷贝Binary。在大多数情况下，Binary额外分配的空间也会在这个时候也被回收掉。 如果将Binary作为消息发送到其他进程或端口，则binary对象会缩小，任何进一步的append操作都会将Binary数据复制到新的Binary中。例如，在下面的代码，第3行的Bin1将会被复制： 123Bin1 = &lt;&lt;Bin0,...&gt;&gt;,PortOrPid ! Bin1,Bin = &lt;&lt;Bin1,...&gt;&gt; %% Bin1 will be COPIED 同样的情况一样会发生，如果将Binary插入到Ets表中、使用erlangport_command/2将其发送到端口、或者将其传递给NIF中的enif_inspect_binary。 匹配Binary也会导致其缩小，下一个append操作将会复制Binary数据： 123Bin1 = &lt;&lt;Bin0,...&gt;&gt;,&lt;&lt;X,Y,Z,T/binary&gt;&gt; = Bin1,Bin = &lt;&lt;Bin1,...&gt;&gt; %% Bin1 will be COPIED 原因是match context包含直接指向二进制数据的指针。 如果一个进程简单地保留Binary（在“循环数据”或进程字典中），垃圾回收器最终可以收缩这个Binary。如果只保留一个这样的Binary，则不会收缩。如果该进程后续append到已收缩的Binary中，则Binary对象将被重新分配，以使数据被加上。 匹配Binary重新看下文章开头的匹配例子： 123my_binary_to_list(&lt;&lt;H,T/binary&gt;&gt;) -&gt; [H|my_binary_to_list(T)];my_binary_to_list(&lt;&lt;&gt;&gt;) -&gt; []. 第一次调用my_binary_to_list/1时，会创建match context。match context指向Binary的第一个字节。匹配1个字节，并更新match context以指向Binary的第二个字节。 在此时，创建一个sub binary似乎是有意义的，但是在这个特定的例子中编译器知道每次匹配后会马上调用一个函数（在这个例子中，是my_binary_to_list/1本身），这会导致要创建一个新的match context然后丢弃sub binary。 因此，my_binary_to_list/1使用match context而不是使用sub binary调用自身。初始化匹配操作的指令当它看到它被传递给match context而不是sub binary时基本上什么都不做。 当到达Binary的末尾并且第二个子句匹配时，match context将被简单地丢弃（在下一个垃圾回收中被移除，因为不再有任何引用）。 总而言之，my_binary_to_list/1只需要创建一个match context，而不需要sub binary。 注意，当遍历完整个Binary后，my_binary_to_list/1中的match context被丢弃。如果迭代在Binary结束之前停止，会发生什么？优化还会生效吗？ 123456after_zero(&lt;&lt;0,T/binary&gt;&gt;) -&gt; T;after_zero(&lt;&lt;_,T/binary&gt;&gt;) -&gt; after_zero(T);after_zero(&lt;&lt;&gt;&gt;) -&gt; &lt;&lt;&gt;&gt;. 答案是依然生效，编译器将在第二个子句中删除sub binary的构建： 1234...after_zero(&lt;&lt;_,T/binary&gt;&gt;) -&gt; after_zero(T);... 但是它会生成在第一个子句中构建sub binary的代码： 123after_zero(&lt;&lt;0,T/binary&gt;&gt;) -&gt; T;... 因此，after_zero/1将构建一个match context和一个sub binary（如果它传进一个包含0的binary）。 以下代码也将进行优化： 123456all_but_zeroes_to_list(Buffer, Acc, 0) -&gt; &#123;lists:reverse(Acc),Buffer&#125;;all_but_zeroes_to_list(&lt;&lt;0,T/binary&gt;&gt;, Acc, Remaining) -&gt; all_but_zeroes_to_list(T, Acc, Remaining-1);all_but_zeroes_to_list(&lt;&lt;Byte,T/binary&gt;&gt;, Acc, Remaining) -&gt; all_but_zeroes_to_list(T, [Byte|Acc], Remaining-1). 编译器将删除第二和第三个子句中的sub binary的构建，并向第一个子句添加一个指令，该指令将Buffer从match context转换成sub binary（如果Buffer已经是binary，则不执行任何操作）。 在开始认为编译器可以优化任何Binary模式匹配之前，以下函数不能由编译器进行优化（至少当前是这样的）： 123456non_opt_eq([H|T1], &lt;&lt;H,T2/binary&gt;&gt;) -&gt; non_opt_eq(T1, T2);non_opt_eq([_|_], &lt;&lt;_,_/binary&gt;&gt;) -&gt; false;non_opt_eq([], &lt;&lt;&gt;&gt;) -&gt; true. 之前提到，如果编译器知道Binary不会被共享，则会延迟创建sub binary。在当前的这种情况下，编译器无法知道。 很快，下面的章节将解释如何重写non_opt_eq/2，以便可以应用延迟sub binary的优化，更重要的是，还能发现你的代码是否可以优化。 bin_opt_info选项使用bin_opt_info选项可以让编译器打印大量有关二进制优化的信息。 erlc +bin_opt_info Mod.erl 请注意，bin_opt_info不能是能永久添加到Makefile中的选项，因为它生成的所有信息都不能被删除。因此，通过环境选择在大多数情况下是最实际的方法。 为了更准确地说明警告所引用的代码，以下示例中的警告将作为注释引用到它们所引用的子句之后插入，例如： 12345678after_zero(&lt;&lt;0,T/binary&gt;&gt;) -&gt; %% NOT OPTIMIZED: sub binary is used or returned T;after_zero(&lt;&lt;_,T/binary&gt;&gt;) -&gt; %% OPTIMIZED: creation of sub binary delayed after_zero(T);after_zero(&lt;&lt;&gt;&gt;) -&gt; &lt;&lt;&gt;&gt;. 上述代码说明第一个匹配没有优化，第二个匹配会被优化。 让我们重新回顾一下无法优化的代码的例子，并找出原因： 123456789101112non_opt_eq([H|T1], &lt;&lt;H,T2/binary&gt;&gt;) -&gt; %% INFO: matching anything else but a plain variable to %% the left of binary pattern will prevent delayed %% sub binary optimization; %% SUGGEST changing argument order %% NOT OPTIMIZED: called function non_opt_eq/2 does not %% begin with a suitable binary matching instruction non_opt_eq(T1, T2);non_opt_eq([_|_], &lt;&lt;_,_/binary&gt;&gt;) -&gt; false;non_opt_eq([], &lt;&lt;&gt;&gt;) -&gt; true. 编译器发出两个警告。INFO警告指的是函数non_opt_eq/2作为被调用者，表示任何调用non_opt_eq/2的函数都不能进行延迟sub binary优化。还有一个建议来改变参数顺序。第二个警告（恰好是指同一行）是指sub binary本身的构造。 下面的另一个例子将显示INFO和NOT OPTIMIZED警告之间的区别，这些警告有些清晰，但是让我们先来试一下改变参数顺序的建议： 1234567opt_eq(&lt;&lt;H,T1/binary&gt;&gt;, [H|T2]) -&gt; %% OPTIMIZED: creation of sub binary delayed opt_eq(T1, T2);opt_eq(&lt;&lt;_,_/binary&gt;&gt;, [_|_]) -&gt; false;opt_eq(&lt;&lt;&gt;&gt;, []) -&gt; true. 编译器给出以下代码片段的警告： 1234567match_body([0|_], &lt;&lt;H,_/binary&gt;&gt;) -&gt; %% INFO: matching anything else but a plain variable to %% the left of binary pattern will prevent delayed %% sub binary optimization; %% SUGGEST changing argument order done;... 这个警告意味着如果有一个对match_body/2的调用（来自match_body/2中的另一个子句或另一个函数），那么延迟的子二进制优化是不可能的。在二进制匹配结束处的任何地方将发生更多警告，并作为match_body/2的第二个参数传递，例如： 1234match_head(List, &lt;&lt;_:10,Data/binary&gt;&gt;) -&gt; %% NOT OPTIMIZED: called function match_body/2 does not %% begin with a suitable binary matching instruction match_body(List, Data). 未使用的变量编译器能够算出一个变量是否未被使用。然后为以下每个功能生成相同的代码： 12345678count1(&lt;&lt;_,T/binary&gt;&gt;, Count) -&gt; count1(T, Count+1);count1(&lt;&lt;&gt;&gt;, Count) -&gt; Count.count2(&lt;&lt;H,T/binary&gt;&gt;, Count) -&gt; count2(T, Count+1);count2(&lt;&lt;&gt;&gt;, Count) -&gt; Count.count3(&lt;&lt;_H,T/binary&gt;&gt;, Count) -&gt; count3(T, Count+1);count3(&lt;&lt;&gt;&gt;, Count) -&gt; Count. 在每次迭代中，二进制中的前8位将被跳过，不匹配。 历史笔记R12的Binary处理显着改善。因此R11B中执行高效的代码在R12B中可能不是那么高效，反之亦然，此efficiency-guide的较早版本包含了有关R11B中二进制处理的一些信息。","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"efficiency_guide:需要注意的模块和BIF","slug":"efficiency-guide-需要注意的模块和BIF","date":"2017-10-14T04:38:02.000Z","updated":"2018-05-04T15:15:55.984Z","comments":true,"path":"2017/10/14/efficiency-guide-需要注意的模块和BIF/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/14/efficiency-guide-需要注意的模块和BIF/","excerpt":"","text":"以下就是要注意的模块和BIF，这些内容之前大多数知道，就setelement/3和size/1的使用优化是不知道的，值得一读 Timer模块使用erlang:send_after/3和erlang:start_timer/3创建定时器比使用Timer模块更有效率。Timer模块使用单独的进程来管理定时器。如果许多进程在同时创建和取消定时器，这个进程很容易成为瓶颈。此外Timer模块的有些函数是不通过单一的进程实现的（如定时器：tc/3或定时器：sleep/1），因此这些函数是可以放心使用的。 list_to_atom/1Atom是不进行内存回收的，一旦一个Atom被创建，它永远不会被移除。如果Atom的数量到达虚拟机的上限（默认1,048,576）的话，虚拟机将会奔溃。 因此，在一个连续运行的系统中，将任意输入字符串转换成Atom是危险的。如果只允许某些定义好的原子作为输入，使用list_to_existing_atom/1可以用来防范拒绝服务攻击。 使用list_to_atom/1构建一个可以被传入apply/3函数的atom，效率是比较低的，不推荐在需要运行很快的代码中使用: 1apply(list_to_atom(“some_prefix”+ + Var), foo, Args) length/1与tuple_size/1、byte_size/1和bit_size/1的O(1)时间复杂度不同，length/1执行的时间与List的长度成正比为O(n) 。通常，没有必要担心length/1的速度，因为它是用C语言很高效的实现的，但是如果List很长，为了避免O(n)的时间复杂度。在一些使用场景中length/1可以被模式匹配来代替,如下： 12foo(L) when length(L) &gt;= 3 -&gt; ... 可以被写成模式匹配的方式 12foo([_,_,_|_]=L) -&gt; ... 上面两段代码的区别是：如果L不是的列表，length(L)将会出错，而第二段代码中将不能正确匹配。 setelement/3setelement/3会复制其修改的tuple。因此，使用setelement/3循环更新一个tuple的不同字段，会每次创建一个tuple的新副本。 有一种情况是可以例外的，如果编译器清楚地知道，破坏性地更新tuple会产生tuple复制相同的结果，那么对setelement/3的调用将被替换为一个特殊的破坏性设置指令。在以下代码中，第一个setelement/3调用复制该tuple并修改第9个元素： 1234multiple_setelement(T0) -&gt; T1 = setelement(9, T0, bar), T2 = setelement(7, T1, foobar), setelement(5, T2, new_value). 后面两个setelement/3调用将该复制的tuple的d第7和第5个元素也修改了，不再复制新的tuple。 要实现上述的优化，必须满足以下条件： 索引必须是整数文字，而不是变量或表达式。 索引必须按降序给出。 在连续的setelement/3调用之间不能有任何其他函数调用。 从一个setelement/3调用返回的元组只能在随后的setelement/3调用中使用。 如果代码不能像multi_setelement/1示例中那样被构造，那么修改大元组中的多个元素的最好方法是将元组转换为列表，修改列表，并将其转换回元组。 size/1size/1可以用来返回tuple和binary的大小。但是如果使用tuple_size/1和byte_size/1的话，能为编译器和运行时系统提供了更多优化机会。另一个优点是能给了Dialyzer提供更多的类型信息。 split_binary/2使用模式匹配而不是调用split_binary/2函数来分割二进制通常更有效率。此外，混合使用比特语法匹配和split_binary/2会使比特语法匹配的一些优化失效。 12&lt;&lt;Bin1:Num/binary,Bin2/binary&gt;&gt; = Bin %% 推荐&#123;Bin1,Bin2&#125; = split_binary(Bin, Num) %% 不推荐 运算符 “- -““- -“ 运算符具有与其操作数长度乘积成比例的时间复杂度（O(m*n)）。这意味着如果该操作符的两个操作数都是长列表，那么操作者非常慢： 12345678HugeList1 -- HugeList2%% 上述操作应该被替换成下面的操作HugeSet1 = ordsets:from_list(HugeList1),HugeSet2 = ordsets:from_list(HugeList2),ordsets:subtract(HugeSet1, HugeSet2)%% 如果在意列表的原始顺序的话，可以退换成如下的操作Set = gb_sets:from_list(HugeList2),[E || E &lt;- HugeList1, not gb_sets:is_element(E, Set)] 注意：如果列表包含重复的元素(HugeList2中出现一个元素在HugeList1中删除了所有出现的元素)，则该代码的行为与“- -”不同。另外，这个代码比较了使用“==”运算符的列表元素，而“- -”使用“=:=”运算符。如果这个区别很重要，那么可以使用set代替gb_set，但是在长列表中set:from_list/1比gb_sets:from_list/1慢得多。 使用“- -”运算符从列表中删除一个元素不会有性能问题：HugeList1 – [Element] 。","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"efficiency_guide:七个Erlang性能的误区","slug":"efficiency-guide-七个Erlang性能的误区","date":"2017-10-14T04:35:14.000Z","updated":"2018-05-04T15:15:55.983Z","comments":true,"path":"2017/10/14/efficiency-guide-七个Erlang性能的误区/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/14/efficiency-guide-七个Erlang性能的误区/","excerpt":"","text":"这些内容是有些是由于Erlang版本变化做的一些优化，和之前的有些要点有些出入，快速的扫一下即可 尾递归总是比普通递归来的快这个说法在R12B之前是真的。在R7B之前更是如此。但是现在普通递归通常使用与尾递归相同的内存量，通常不可能预测尾递归或身体递归版本是否更快。因此，使用使代码更清洁的版本就可以了（通常是普通递归的版本）。但是死循环还是要使用尾递归，防止内存耗尽。 “++” 操作总是不好的如果是这样[H] ++ Tail 使用 “++” 操作的话，没有什么不好的，编译器会自动把该操作转换成[H| Tail]。 字符串操作很慢如果不正确地使用字符串，字符串操作速度可能很慢。在Erlang中，需要更多地思考如何使用字符串并选择适当的字符表示。如果使用正则表达式，请使用STDLIB中的re模块，而不是过时的regexp模块。 修复Dets文件非常慢Dets文件的修复时间与文件中的记录数成正比，虽然Dets文件修复以前很慢，但是Dets的实现已被大量改写和改进。 BEAM是一个基于堆栈的字节码虚拟机（因此比较慢）BEAM是一个基于寄存器的虚拟机。它有1024个虚拟寄存器，用于保存临时值，并在调用函数时传递参数。需要在函数调用中使用的变量将保存到堆栈中。 BEAM是一个线程代码解释器。每个指令是直接指向可执行C代码的字，使得指令调度非常快。 当变量不被使用时，使用“_”来加快程序速度这个在R6B版本之前是这样的，但是在这个版本之后，编译器能够自动识别不使用的变量，所以用不用“_”都一样。 NIF总是能使你的程序更快将Erlang代码重写为NIF以使其更快，应该被视为最后的手段。使用NIF肯定有风险，但是不能保证程序能更快。在每个NIF调用中进行太多的工作会降低VM的响应能力。做太少的工作可能意味着NIF中更快处理的优势被调用NIF并检查参数的开销所抵消了。所以在写NIF之前，请务必阅读 Long-running NIFs 。","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"Erlang文档的efficiency_guide总结","slug":"Erlang文档的efficiency-guide总结","date":"2017-10-13T15:07:04.000Z","updated":"2018-05-04T15:15:55.981Z","comments":true,"path":"2017/10/13/Erlang文档的efficiency-guide总结/","link":"","permalink":"https://lintingbin2009.github.io/2017/10/13/Erlang文档的efficiency-guide总结/","excerpt":"","text":"之前刚开始学习的Erlang的时候稍微看过这个教程，但是没有看全，发现这个教程还涵盖了挺多的信息的，今天把这个教程看完，顺便做一下总结，教程原版地址 本来是想把所有的总结写在一篇文章里面的，但是由于篇幅比较大，所以就把所有的总结分为以下几篇文章： efficiency_guide:七个Erlang性能的误区 efficiency_guide:需要注意的模块和BIF efficiency-guide:Binary的构建和匹配 efficiency-guide:List处理 efficiency-guide:函数 efficiency-guide:表和数据库 efficiency-guide:进程 最后再提两个误区： 匿名函数很慢 匿名函数过去很慢，慢于apply/3。最初，使用编译器技巧，普通元组，apply/​3和大量的精巧方法实现了匿名函数。但那是历史。匿名函数在R6B中给出了自己的数据类型，并在R7B中进一步优化。现在，一个匿名函数的调用开销大概在调用本地函数和apply/3的开销之间。 列表推导很慢 以前通过匿名函数实现列表推导，而在过去匿名函数确实很慢。 如今，编译器将列表推导重写成一个普通的递归函数。","categories":[{"name":"Erlang Efficiency Guide","slug":"Erlang-Efficiency-Guide","permalink":"https://lintingbin2009.github.io/categories/Erlang-Efficiency-Guide/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"怎么实现一个Sublime的自动补全插件","slug":"怎么实现一个Sublime的自动补全插件","date":"2017-09-03T05:45:49.000Z","updated":"2018-05-04T15:15:55.989Z","comments":true,"path":"2017/09/03/怎么实现一个Sublime的自动补全插件/","link":"","permalink":"https://lintingbin2009.github.io/2017/09/03/怎么实现一个Sublime的自动补全插件/","excerpt":"","text":"使用Erlang开发了快三年的游戏了，一直使用的是Sublime编辑器，也就这样没有自动补全的情况下使用了三年，本来打算切换到有Erlang自动补全的Ide的，但是在Sublime上面开发了那么久，切换到其他的编辑器觉得很不习惯，所以就自己写了一个Erlang的自动补全的插件，点这里可以看到我的插件 Sublime插件是用Python写的，所以打算开发Sublime插件的话要稍微去学习下Python，不用学的很深入，懂得基本的语法就可以愉快的开始开发插件了。我之前的入门教程看的是creating-sublime-text-3-plugins-part-1，如果打算开发Sublime插件的话，看这篇文章就可以写一个简单的Sublime插件的Demo。这个网址api_reference可以查看开发Sublime插件所提供的各种API。 我写Erlang自动补全代码和自动跳转的原理是在打开Sublime的时候，扫描所有Erlang的源代码和Sublime中已经打开的所有的Erlang代码，然后利用正则表达式匹配来找出所有函数和模块所在的文件和位置，把这些信息都写入到Sqlite数据库中，然后在用户在编写Erlang源代码的时候提供补全的函数和模块。当用户把鼠标指向某个函数的时候，在Sqlite数据库中查询相应的函数所在的文件和位置，当用户选中的时候打开该文件并且定位到文件的相应的位置。具体的代码可以在点这里可以看到我的插件这里查看。当写好一个插件后我们最好能把插件放到Package Control中，这样用户安装和升级插件就会非常的方便，通过这个submitting_a_package教程能够顺利的提交自己的插件到Package Control中。 自己写一个小插件有时候还是可以学到一点东西的，通过这次编写自动补全的插件，让我对正则表达式稍微熟悉了一点。","categories":[{"name":"教程","slug":"教程","permalink":"https://lintingbin2009.github.io/categories/教程/"}],"tags":[{"name":"Sublime","slug":"Sublime","permalink":"https://lintingbin2009.github.io/tags/Sublime/"},{"name":"插件","slug":"插件","permalink":"https://lintingbin2009.github.io/tags/插件/"}]},{"title":"mochiweb的x-forwarded-for实现引发的线上掉单","slug":"mochiweb的x-forwarded-for实现引发的线上掉单","date":"2017-08-18T13:13:44.000Z","updated":"2018-05-04T15:15:55.985Z","comments":true,"path":"2017/08/18/mochiweb的x-forwarded-for实现引发的线上掉单/","link":"","permalink":"https://lintingbin2009.github.io/2017/08/18/mochiweb的x-forwarded-for实现引发的线上掉单/","excerpt":"","text":"记录一次线上充值服的掉单问题，同时学习下什么是x-forwarded-for 掉单原因？因为充值服都设有白名单，如果充值请求的机器的IP不在白名单里面的话会被视为非法IP，在掉单期间，线上的充值服发现有大量的100.116. . 的非法IP的访问，之后在网上一查，原来100.64.0.0/10也是属于内网IP的。我们的充值服务器使用了负载均衡，所以100.116..的IP应该是负载均衡机器的内网IP，同时由于我们充值服务器使用的是mochiweb的服务器，所以第一时间查看了下mochiweb获取IP的源代码： 123456789101112131415161718192021222324252627282930313233343536get(peer, &#123;?MODULE, [Socket, _Opts, _Method, _RawPath, _Version, _Headers]&#125;=THIS) -&gt; case mochiweb_socket:peername(Socket) of &#123;ok, &#123;Addr=&#123;10, _, _, _&#125;, _Port&#125;&#125; -&gt; case get_header_value(\"x-forwarded-for\", THIS) of undefined -&gt; inet_parse:ntoa(Addr); Hosts -&gt; string:strip(lists:last(string:tokens(Hosts, \",\"))) end; %% Copied this syntax from webmachine contributor Steve Vinoski &#123;ok, &#123;Addr=&#123;172, Second, _, _&#125;, _Port&#125;&#125; when (Second &gt; 15) andalso (Second &lt; 32) -&gt; case get_header_value(\"x-forwarded-for\", THIS) of undefined -&gt; inet_parse:ntoa(Addr); Hosts -&gt; string:strip(lists:last(string:tokens(Hosts, \",\"))) end; &#123;ok, &#123;Addr=&#123;192, 168, _, _&#125;, _Port&#125;&#125; -&gt; case get_header_value(\"x-forwarded-for\", THIS) of undefined -&gt; inet_parse:ntoa(Addr); Hosts -&gt; string:strip(lists:last(string:tokens(Hosts, \",\"))) end; &#123;ok, &#123;&#123;127, 0, 0, 1&#125;, _Port&#125;&#125; -&gt; case get_header_value(\"x-forwarded-for\", THIS) of undefined -&gt; \"127.0.0.1\"; Hosts -&gt; string:strip(lists:last(string:tokens(Hosts, \",\"))) end; &#123;ok, &#123;Addr, _Port&#125;&#125; -&gt; inet_parse:ntoa(Addr); &#123;error, enotconn&#125; -&gt; exit(normal) end; 从上面的代码可以看出，如果在服务器内网里面使用了代理服务器之后，mochiweb是能够自动获取原始的访问IP。但是仅限内网代理服务器的IP是一些常见的内网IP，100.64.0.0/10段的IP地址并不包括在里面，所以这时候获取的IP就不是原始IP，而是负载均衡机器的内网IP。 内网IP段有哪些？10.0.0.0/810.0.0.0 - 10.255.255.255 172.16.0.0/12172.16.0.0 - 172.31.255.255 192.168.0.0/16192.168.0.0 - 192.168.255.255 以上三个网段分别属于A、B、C三类IP地址 100.64.0.0/10100.64.0.0 - 100.127.255.255由运营商使用的私网IP段，随着IPv4地址池的耗光，会有更多用户被分配到这个网段。我们的线上掉单问题就是因为阿里云把内网IP切换到这个网段造成的。 http协议头标：x-forwarded-forX-Forwarded-For(XFF)是用来识别通过HTTP代理或负载均衡方式连接到Web服务器的客户端最原始的IP地址的HTTP请求头字段。 Squid 缓存代理服务器的开发人员最早引入了这一HTTP头字段，并由IETF在Forwarded-For HTTP头字段标准化草案中正式提出。 总结这次的掉单问题算起来应该算是一个不太能够发现的坑，主要是依赖第三方库的实现，我们这边相关的同事已经把修复代码提交pull request到mochiweb的github主页了，防止有更多的人碰到这个坑。","categories":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/categories/Erlang/"}],"tags":[{"name":"mochiweb","slug":"mochiweb","permalink":"https://lintingbin2009.github.io/tags/mochiweb/"},{"name":"x-forwarded-for","slug":"x-forwarded-for","permalink":"https://lintingbin2009.github.io/tags/x-forwarded-for/"}]},{"title":"基于Erlang的全区服分数竞技场的设计的优化","slug":"基于Erlang的全区服分数竞技场的设计的优化","date":"2017-07-13T13:47:51.000Z","updated":"2018-05-04T15:15:55.989Z","comments":true,"path":"2017/07/13/基于Erlang的全区服分数竞技场的设计的优化/","link":"","permalink":"https://lintingbin2009.github.io/2017/07/13/基于Erlang的全区服分数竞技场的设计的优化/","excerpt":"","text":"在上一篇文章Erlang动态代码载入小实验中，我提到的竞技场设计中存在一些性能问题，这篇文章主要是针对上一篇文章提到的性能问题在整体设计方案不进行大改的情况下进行优化。 主要性能问题回顾上一篇文章，我们知道之前的设计方案的主要问题是：频繁的在分数ETS中拿取和更新新的玩家List，而且该List的大小有可能是几十万的级别的，主要的性能问题是ETS和排行榜进程之间的数据拷贝。 在同一分数段的所有玩家都用List来存储的话，每次在一个分数中增加一个玩家的代价是，先从ets中lookup拿出所有这一分数的玩家，然后在这个列表中增加新的玩家，最后再把新的玩家列表更新回去，删除也是如此。如果每个分数的玩家列表都不是很大的话，这个应该问题也不会很大，但是由于同一分数段的玩家比较多，所以这个方案的性能就很差了。 优化一既然主要的性能问题出在List的更新和删除的操作，所以这个优化方案的主要方法是把List替换成ETS，当List的长度大于N(N可以自己设置，比如100、200之类)时，同一分数的所有玩家都存储在ETS中，这样在一个分数的玩家列表中增加一个玩家也只是在ETS中增加一个玩家ID而已，删除一个玩家的话，也只是在ETS中删除一个玩家ID，这两个操作都非常的快。 经过这一次的优化，竞技场玩家已经可以在正式环境中上线，但是在游戏最高峰的时间段，玩家还是会有点卡，此时游戏服务器（16核）的cpu几乎全部跑满，所以还需要进一步的优化。 优化二通过上一次优化我们知道，服务器在高峰的时候几乎把cpu全部跑满。这时候我就开始怀疑寻找对手的算法是否有问题，之前寻找对手都是现算的，把玩家的对手的排名算出来，然后在分数的ETS里面开始从头到尾遍历所有分数，找出符合对手排名的玩家。这样子寻找一个玩家的三个对手，大概要遍历分数ETS一千多次，在平时的时候还是非常快的，下面是我用eprof测量的在平时寻找一次对手的一些关键操作的开销： 1234legend_arena_global_rank:query_apprentice_rank/7 389 4.25 290 [ 0.75]legend_arena_global_rank:query_apprentice_key/3 1076 7.45 508 [ 0.47]ets:lookup_element/3 1465 34.31 2340 [ 1.60]ets:prev/2 1459 45.52 3105 [ 2.13] 再下面的是我用eprof测试的在小高峰期寻找一次对手的一些关键操作的开销： 1234legend_arena_global_rank:query_apprentice_rank/7 386 1.16 216 [ 0.56]legend_arena_global_rank:query_apprentice_key/3 1029 2.95 551 [ 0.54]ets:lookup_element/3 1415 39.47 7371 [ 5.21]ets:prev/2 1409 54.38 10157 [ 7.21] 通过上面两次的测量可以看到：分数ETS在大量访问的时候出现了性能下降，本来ets:prev/2操作只需要2.13us，在高峰期居然需要7.21us；本来ets:lookup_element/3操作只需要1.6us，在高峰期居然需要5.21us。 所以这次的主要优化方法是把对分数ETS的访问次数降下来，建立一些排名的缓存，当一个玩家寻找对手的时候直接在缓存中寻找，同时每秒钟刷新一次缓存（确保排名比较正确）。这样不管是在高峰期还是平时，分数ETS都不会有非常明显的访问量的提升。 总结通过这次的优化，我认为不管用什么语言来实现一个系统，都要了解这个语言的优势和劣势，这样才能找出一个合理的解决方案来解决一些比较棘手的问题。","categories":[{"name":"算法设计","slug":"算法设计","permalink":"https://lintingbin2009.github.io/categories/算法设计/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"设置Shell脚本执行错误自动退出","slug":"设置Shell脚本执行错误自动退出","date":"2017-07-06T13:44:10.000Z","updated":"2018-05-04T15:15:55.989Z","comments":true,"path":"2017/07/06/设置Shell脚本执行错误自动退出/","link":"","permalink":"https://lintingbin2009.github.io/2017/07/06/设置Shell脚本执行错误自动退出/","excerpt":"","text":"这是一篇备忘记录，以后再写Shell脚本的时候需要注意！ 之前项目使用Jenkins打包的时候，有时候因为一些错误的提交，导致出包的时候编译失败，从而导致打包出来的包里面只有部分的代码，这是因为我们写的Shell脚本没有对每条Shell命令的结果进行检查，不管执行结果是否成功都会继续往下执行。所以即使我们在编译环节有错误产生，打包的脚本还是会继续执行后面的打包指令。所以必须让脚本在某条命令执行失败的时候停止执行后续的指令。在Shell脚本中加入： #!/bin/bash -e 或者 set -e 就能够让脚本在有错误的时候退出。下面是网上查的拓展： 使用set -e123456789你写的每一个脚本的开始都应该包含set -e。这告诉bash一但有任何一个语句返回非真的值，则退出bash。 使用-e的好处是避免错误滚雪球般的变成严重错误，能尽早的捕获错误。更加可读的版本：set -o errexit 使用-e把你从检查错误中解放出来。如果你忘记了检查，bash会替你做这件事。不过你也没有办法使用$? 来获取命令执行状态了，因为bash无法获得任何非0的返回值。你可以使用另一种结构，使用command 使用command123456789101112131415if [ \"$?\"-ne 0]; then echo \"command failed\"; exit 1; fi \"可以替换成： command || echo \"command failed\"; exit 1; （这种写法并不严谨，我当时的场景是执行ssh \"commond\"，所以可以返回退出码后面通过[ #？ -eq 0 ]来做判断，如果是在shell中无论成功还是失败都会exit）修改如下（谢谢评论的朋友指正）command || （echo \"command failed\"; exit 1） ; 或者使用： if ! command; then echo \"command failed\"; exit 1; fi","categories":[{"name":"备忘","slug":"备忘","permalink":"https://lintingbin2009.github.io/categories/备忘/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://lintingbin2009.github.io/tags/Shell/"}]},{"title":"基于Erlang的全区服分数竞技场的设计","slug":"基于Erlang的全区服分数竞技场的设计","date":"2017-06-19T13:38:56.000Z","updated":"2018-05-04T15:15:55.988Z","comments":true,"path":"2017/06/19/基于Erlang的全区服分数竞技场的设计/","link":"","permalink":"https://lintingbin2009.github.io/2017/06/19/基于Erlang的全区服分数竞技场的设计/","excerpt":"","text":"这篇文章主要介绍我之前开发的一个基于Erlang的全区服分数竞技场的设计思路，同时会给出该设计的在现实项目中出现的问题，并且在后续的几篇文章中优化设计。 关键需求该分数竞技场的实际需求比较多，这边只列举对我们设计有影响的几个关键需求，以下是该分数竞技场的关键需求： 每个玩家拥有一个分数，该分数大概是6000以内的数字。 玩家要实时知道自己的分数和排名。 玩家可以手动刷新自己的对手，玩家的对手由于玩家的排名乘以30%、60%、90%左右的排名的玩家随机出来。比如一个1000名的玩家，他的对手可能是由278、632、945名次的玩家组成。 玩家挑战对手，如果战胜，则玩家自己加分，对手扣分，反之亦然。 该竞技场每天在某一时间点进行结算发奖。 玩家数量级玩家的数量级有两个，分别是测试环境和正式环境： 测试环境的玩家帐号有几十万，日活是2万左右。 正式环境的玩家帐号有几百万，日活是60万左右。 设计思路下面介绍该分数竞技场的具体设计思路: 由于该竞技场是采用分数来排名，而且分数的区间比较小，所以我想到了使用桶排序来对玩家分数排名。 由于该竞技场是所有玩家共同访问的，所以打算用ETS来实现这个桶排序。 由于分数需要是有序存储的，所以该ETS为ordered_set类型，而且Key为分数，Value有两个字段： list——存储该分数的所有玩家的key cnt——存储该分数的玩家总数 玩家的排名为：从该分数ETS分数最大的元素开始遍历，遍历到玩家所在的分数的前一个分数，在遍历的同时累加遍历到的cnt值，玩家的排名为累加值加1，同一分数玩家的排名一致。例如：玩家分数为5000分，在5000分之前有100个5020分，1个5500分，则玩家为第102名。 当玩家进行一场挑战的时候，把玩家key从他原来的分数list里面移除，并且该分数的cnt-1；同时把玩家key加入到新的分数的list，并且该新分数的cnt+1；对手的分数改变也是进行同样的操作；这些操作在gen_server中进行，确保数据不会被脏写。 运行结果 该设计在测试环境中测试通过了，而且没有发现什么异常。 在正式环境中，只有少量玩家访问的情况下，访问时间到达几秒的级别，只能暂时关闭该功能进行优化。 主要问题这边列举两个比较严重的问题： 同一分数的玩家数量很多，同一分数最多的玩家有50万人,使用list来存储玩家的key，每次对这个list增删代价巨大。 由于ETS是另外一个单独的进程，每次从ETS中拿一个50万人的list，然后再把新的list更新回去，代价同样巨大。 主要优化目标由于留给优化的时间比较短，所以要在原有的设计思路下对该分数竞技场进行优化，达到能够上线的标准。","categories":[{"name":"算法设计","slug":"算法设计","permalink":"https://lintingbin2009.github.io/categories/算法设计/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"Markdown语法速记","slug":"Markdown语法速记","date":"2017-06-17T14:44:26.000Z","updated":"2018-05-04T15:15:55.981Z","comments":true,"path":"2017/06/17/Markdown语法速记/","link":"","permalink":"https://lintingbin2009.github.io/2017/06/17/Markdown语法速记/","excerpt":"","text":"有时候自己也会忘记Markdown的语法，在这边做一个备忘，以后找起来比较方便，这边记录的是最基本的Markdown语法。 粗体和斜体1_下划线是斜体_ 下划线是斜体 1**两个星是粗体** 两个星是粗体 1**_粗体斜体一起用_** 粗体斜体一起用 六种标题几个#号代表标题几,#号后面有空格 123456# 标题1## 标题2### 标题3#### 标题4##### 标题5###### 标题6 链接123456这是一个 [普通的链接方式](https://www.github.com)这是一个 [引用的链接方式][another place].这还是一个 [引用的链接方式][another-link].[another place]: https://www.github.com[another-link]: https://www.google.com 这是一个 普通的链接方式 这是一个 引用的链接方式. 这还是一个 引用的链接方式. 图片1234![我的头像](https://lintingbin2009.github.io/img/avatar.jpg)![又是一个头像][other][other]: https://lintingbin2009.github.io/img/avatar.jpg 我的头像 引用123&gt;在要被引用的段落或者行前面加大括号&gt;&gt;即使是空行也要加一下，保持一致 在要被引用的段落或者行前面加大括号 即使是空行也要加一下，保持一致 列表123451. 有序用数字 继续保持缩进,只需加空格2. 有序用数字 * 无序用星号 * 还可再缩进,只需再加空格 有序用数字 继续保持缩进,只需加空格 有序用数字 无序用星号 还可再缩进,只需再加空格 段落12我在逗号后加了两个空格, 所以不在一行 我在逗号后加了两个空格,所以不在一行","categories":[{"name":"教程","slug":"教程","permalink":"https://lintingbin2009.github.io/categories/教程/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"https://lintingbin2009.github.io/tags/Markdown/"}]},{"title":"Erlang中由rpc:cast错误引起的对error_logger的研究","slug":"Erlang中由rpc-cast错误引起的对error-logger的研究","date":"2017-06-15T14:45:28.000Z","updated":"2018-05-04T15:15:55.978Z","comments":true,"path":"2017/06/15/Erlang中由rpc-cast错误引起的对error-logger的研究/","link":"","permalink":"https://lintingbin2009.github.io/2017/06/15/Erlang中由rpc-cast错误引起的对error-logger的研究/","excerpt":"","text":"之所以会写这篇文章，是因为rpc:cast函数的使用超出了我的理解范围，本来我的理解是：如果rpc:cast执行失败的话，是不会有任何报错的。但是由于最近的线上存在两个版本的代码进行的互相调用引发了一些报警，让我好奇rpc:cast和error_logger是怎么工作的。 rpc:cast 是怎么工作的？我们在查阅rpc的源代码的时候可以发现以下代码： 12345678910111213141516-define(NAME, rex)....handle_cast(&#123;cast, Mod, Fun, Args, Gleader&#125;, S) -&gt; spawn(fun() -&gt; set_group_leader(Gleader), apply(Mod, Fun, Args) end), &#123;noreply, S&#125;;...cast(Node, Mod, Fun, Args) when Node =:= node() -&gt; catch spawn(Mod, Fun, Args), true;cast(Node, Mod, Fun, Args) -&gt; gen_server:cast(&#123;?NAME,Node&#125;, &#123;cast,Mod,Fun,Args,group_leader()&#125;), true.... 从上面的代码我们可以看到，rpc:cast的时候如果目标node和本地node一样的话就会直接spawn一个进程处理，如果是远程的话，则会调用一个名字为rex的gen_server到远程的服务器上执行，远程的服务器同样也是spawn一个进程来处理。如果一个服务器是为其他服务器提供服务的（通过rpc模块），那么这个服务器的rex应该会是最繁忙的。通过上面的分析我们知道rpc:cast的错误是spawn函数通知error_logger的。 spawn出来的进程执行遇到错误怎么处理？我自己试验了下，比如我自己在shell里面执行spawn(fun() -&gt; 1 = 2 end).语句的话，error_logger就会收到如下的一个错误事件： 123&#123;error,&lt;113291.32.0&gt;, &#123;emulator,\"~s~n\", [\"Error in process &lt;0.13313.2075&gt; on node 'all_in_one_33000@192.168.1.102' with exit value: &#123;&#123;badmatch,2&#125;,[&#123;erl_eval,expr,3,[]&#125;]&#125;\\n\"]&#125;&#125; 为了明白上述的情况为什么会发生，我在Erlang邮件列表里面找到两个类似的问题，可以解答我的疑问： [erlang-questions] An answer: how does SASL know that a process died? [erlang-questions] error_logger events sent by emulator 简单的总结上面的两个问题，spawn执行的程序遇到异常的话，是由虚拟机的C语言代码向error_logger发送的错误事件。 error_logger 是怎么工作的？error_logger是Erlang的错误记录器，由gen_event实现，在Erlang系统中会有一个注册名为error_logger的事件管理器(event manager)，可以在事件管理器中加入各种处理模块来处理事件。默认的系统中会加入以下两个错误处理模块： 1234567$ erlErlang R16B03 (erts-5.10.4) [source] [64-bit] [smp:12:12] [async-threads:10] [hipe] [kernel-poll:false]Eshell V5.10.4 (abort with ^G)1&gt; gen_event:which_handlers(error_logger).[error_logger,error_logger_tty_h] 简单的说下这两个错误处理模块，首先是error_logger模块，以下是该模块的处理事件的部分代码： 123456789101112131415161718192021handle_event(&#123;Type, GL, Msg&#125;, State) when node(GL) =/= node() -&gt; gen_event:notify(&#123;error_logger, node(GL)&#125;,&#123;Type, GL, Msg&#125;), %% handle_event2(&#123;Type, GL, Msg&#125;, State); %% Shall we do something &#123;ok, State&#125;; %% at this node too ???handle_event(&#123;info_report, _, &#123;_, Type, _&#125;&#125;, State) when Type =/= std_info -&gt; &#123;ok, State&#125;; %% Ignore other info reports herehandle_event(Event, State) -&gt; handle_event2(Event, State)....handle_event2(Event, &#123;1, Lost, Buff&#125;) -&gt; display(tag_event(Event)), &#123;ok, &#123;1, Lost+1, Buff&#125;&#125;;handle_event2(Event, &#123;N, Lost, Buff&#125;) -&gt; Tagged = tag_event(Event), display(Tagged), &#123;ok, &#123;N-1, Lost, [Tagged|Buff]&#125;&#125;;handle_event2(_, State) -&gt; &#123;ok, State&#125;....display2(Tag,F,A) -&gt; erlang:display(&#123;error_logger,Tag,F,A&#125;). 该模块把是本node产生的事件调用erlang:display()输出，把不是本node产生的事件发送到目标node上面，由目标node的error_logger进行处理。 接着是error_logger_tty_h模块，以下是该模块的处理事件的部分代码： 12345678910111213141516171819handle_event(&#123;_Type, GL, _Msg&#125;, State) when node(GL) =/= node() -&gt; &#123;ok, State&#125;;handle_event(Event, State) -&gt; write_event(tag_event(Event),io), &#123;ok, State&#125;....write_event(&#123;Time, &#123;error, _GL, &#123;Pid, Format, Args&#125;&#125;&#125;,IOMod) -&gt; T = write_time(maybe_utc(Time)), case catch io_lib:format(add_node(Format,Pid), Args) of S when is_list(S) -&gt; format(IOMod, T ++ S); _ -&gt; F = add_node(\"ERROR: ~p - ~p~n\", Pid), format(IOMod, T ++ F, [Format,Args]) end;...format(IOMod, String) -&gt; format(IOMod, String, []).format(io_lib, String, Args) -&gt; io_lib:format(String, Args);format(io, String, Args) -&gt; io:format(user, String, Args). 该模块把不是该node的事件直接忽略，然后把本node的事件调用io:format输出到终端上面。 除了这两个处理模块，Erlang的sasl应用还提供了三个模块：sasl_report_tty_h、sasl_report_file_h、log_mf_h。log_mf_h模块的功能最为强大，能够把错误写入指定个数的文件中，当文件用完后会自动删除最老的事件以腾出空间记录最新的事件。但是log_mf_h的缺点是记录的是二进制的格式，要查看记录的事件的话，还需要使用sasl提供的rb模块来解析，颇为繁琐。而且该模块没有对单事件的最大上限做保护，如果有超大的事件写入的话，就会导致文件错乱，看不了事件（这个可以自己写代码做保护，我们项目之前就是这样做的）。 当然除了官方提供的处理模块，也可以使用第三方提供的模块。现在我们项目就把所有官方提供的模块都删除掉了，只使用lager提供的error_logger_lager_h模块来处理事件，然后自己编写了一个alarm_handle_error模块用来发送报警。error_logger_lager_h使用文本的方式来记录事件，查看起来比较方便，而且对Erlang内部一些比较难以理解的错误进行翻译，比较容易理解；但是由于使用文本的方式进行记录，没有对事件消息进行格式化，如果消息比较大的话，读起来比较费劲。 error_logger 添加处理模块的注意事项当使用sasl提供的log_mf_h处理模块的时候不能删除系统提供的error_logger模块，不然像rpc:cast通知的事件就不能正常的捕获了，原因如下： 123456%% 当NodeA执行以下函数的时候，在NodeB会接收到一个错误，%% 由于NodeB只有log_mf_h模块，log_mf_h模块会对接收的事件使用sasl:pred/1函数进行过滤%% sasl:pred/1会过滤不是本node的产生的事件，因此该错误被过滤%% 如果这时候NodeB有error_logger模块的话，error_logger模块就会将事件通知NodeA%% 然后NodeA就能使用log_mf_h模块正确记录该错误，该错误记录在NodeA的机器上NodeA: rpc:cast(NodeB, M, ErrorFun, []). lager的error_logger_lager_h模块默认会记录所有的事件，不管该事件是属于哪个Node的，如下： 123%% 当NodeA执行以下函数的时候，在NodeB会接收到一个错误，%% error_logger_lager_h直接记录错误在NodeB的机器上NodeA: rpc:cast(NodeB, M, ErrorFun, []). 最后说两句之前项目使用log_mf_h模块处理事件的配置文件如下： 1234567[&#123;sasl, [ &#123;sasl_error_logger, false&#125;, &#123;errlog_type, error&#125;, &#123;error_logger_mf_dir, \"logs\"&#125;, &#123;error_logger_mf_maxbytes, 1073741824&#125;, % 1GB &#123;error_logger_mf_maxfiles, 10&#125; ]&#125;]. 之前一直觉得errlog_type是控制log_mf_h模块的处理事件级别的参数，这边设置的参数是error，为什么info的信息还会记录下来呢？后面看了下sasl.erl模块的代码，errlog_type和log_mf_h模块根本没有关系，然后回头再看了一下sasl的文档: log_mf_h This error logger writes all events sent to the error logger to disk. Multiple files and log rotation are used. For efficiency reasons, each event is written as a binary. For more information about this handler, see the STDLIB Reference Manual. To activate this event handler, three SASL configuration parameters must be set, error_logger_mf_dir, error_logger_mf_maxbytes, and error_logger_mf_maxfiles. The next section provides more information about the configuration parameters. 文档中all已经加黑了，我居然没看到，以后还得好好认真看文档！","categories":[{"name":"Erlang深入","slug":"Erlang深入","permalink":"https://lintingbin2009.github.io/categories/Erlang深入/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"使用lager为什么要加入编译选项{parse_transform,lager_transform}","slug":"使用lager为什么要加入编译选项-parse-transform-lager-transform","date":"2017-05-30T05:03:05.000Z","updated":"2018-05-04T15:15:55.987Z","comments":true,"path":"2017/05/30/使用lager为什么要加入编译选项-parse-transform-lager-transform/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/30/使用lager为什么要加入编译选项-parse-transform-lager-transform/","excerpt":"","text":"在使用的lager的时候我们需要加入一行编译选项——{parse_transform,lager_transform}，或者是在每个使用lager的文件模块的头部加入一行-compile([{parse_transform, lager_transform}]).，这通常会让我们感觉非常的麻烦，但是大家有没有觉得好奇，为什么使用这个参数呢？ 首先我们看下Erlang文档，在compile模块中有parse_transform参数的相关说明： {parse_transform,Module} Causes the parse transformation function Module:parse_transform/2 to be applied to the parsed code before the code is checked for errors. 通过上面的文档我们知道，在编译的时候使用{parse_transform,Module}参数，会使用Module:parse_transform/2函数对代码进行一次解析转换。接下来我们在lager的源代码目录下可以看到lager_transform.erl的代码文件，里面也有一个parse_transform/2的函数。 1234567891011parse_transform(AST, Options) -&gt; TruncSize = proplists:get_value(lager_truncation_size, Options, ?DEFAULT_TRUNCATION), Enable = proplists:get_value(lager_print_records_flag, Options, true), Sinks = [lager] ++ proplists:get_value(lager_extra_sinks, Options, []), put(print_records_flag, Enable), put(truncation_size, TruncSize), put(sinks, Sinks), erlang:put(records, []), %% .app file should either be in the outdir, or the same dir as the source file guess_application(proplists:get_value(outdir, Options), hd(AST)), walk_ast([], AST). parse_transform/2函数的第一个参数是AST，这个是代码在被编译成二进制前的一种格式The Abstract Format,第二个参数是在编译的时候传入的编译参数，比如要加入一个sink的话不单单要在配置文件里面加入配置，还要在编译参数里面加入{lager_extra_sinks, [audit]}，这样parse_transform/2函数才能在proplists:get_value(lager_extra_sinks, Options, [])的时候获得audit这个sink。 顺着代码往下走，我们看到只有调用的函数的模块名是Sinks中之一的才会被解析转换（lists:member(Module, Sinks)），比如lager:info、lager:error、audit:info、audit:error等函数（audit为我们配置的sink）。 1234567891011121314151617181920212223242526walk_body(Acc, []) -&gt; lists:reverse(Acc);walk_body(Acc, [H|T]) -&gt; walk_body([transform_statement(H, get(sinks))|Acc], T).transform_statement(&#123;call, Line, &#123;remote, _Line1, &#123;atom, _Line2, Module&#125;, &#123;atom, _Line3, Function&#125;&#125;, Arguments0&#125; = Stmt, Sinks) -&gt; case lists:member(Module, Sinks) of true -&gt; case lists:member(Function, ?LEVELS) of true -&gt; SinkName = lager_util:make_internal_sink_name(Module), do_transform(Line, SinkName, Function, Arguments0); false -&gt; case lists:keyfind(Function, 1, ?LEVELS_UNSAFE) of &#123;Function, Severity&#125; -&gt; SinkName = lager_util:make_internal_sink_name(Module), do_transform(Line, SinkName, Severity, Arguments0, unsafe); false -&gt; Stmt end end; false -&gt; list_to_tuple(transform_statement(tuple_to_list(Stmt), Sinks)) end; 最后来到解析转换真正起作用的地方，这边的注释写的很清楚，下面的解析转换等于就是lager:dispatch_log/6里面的内容，如果直接调用lager:dispatch_log/6函数的话，是不需要这样的解析转换的，我对此特地问了下lager的开发者，这样做的话能够提高多少的性能，对方给的答复是能快一倍（图 1-1），因为在log不需要输出的情况下就不需要拷贝内容到外部的函数了，个人觉得一次外部函数调用应该费不了多少时间吧。 1234567891011121314151617181920212223242526272829%% Wrap the call to lager:dispatch_log/6 in case that will avoid doing any work if this message is not elegible for logging%% See lager.erl (lines 89-100) for lager:dispatch_log/6%% case &#123;whereis(Sink), whereis(?DEFAULT_SINK), lager_config:get(&#123;Sink, loglevel&#125;, &#123;?LOG_NONE, []&#125;)&#125; of&#123;'case',Line, &#123;tuple,Line, [&#123;call,Line,&#123;atom,Line,whereis&#125;,[&#123;atom,Line,SinkName&#125;]&#125;, &#123;call,Line,&#123;atom,Line,whereis&#125;,[&#123;atom,Line,?DEFAULT_SINK&#125;]&#125;, &#123;call,Line, &#123;remote,Line,&#123;atom,Line,lager_config&#125;,&#123;atom,Line,get&#125;&#125;, [&#123;tuple,Line,[&#123;atom,Line,SinkName&#125;,&#123;atom,Line,loglevel&#125;]&#125;, &#123;tuple,Line,[&#123;integer,Line,0&#125;,&#123;nil,Line&#125;]&#125;]&#125;]&#125;, %% &#123;undefined, undefined, _&#125; -&gt; &#123;error, lager_not_running&#125;; [&#123;clause,Line, [&#123;tuple,Line, [&#123;atom,Line,undefined&#125;,&#123;atom,Line,undefined&#125;,&#123;var,Line,'_'&#125;]&#125;], [], %% trick the linter into avoiding a 'term constructed but not used' error: %% (fun() -&gt; &#123;error, lager_not_running&#125; end)() [&#123;call, Line, &#123;'fun', Line, &#123;clauses, [&#123;clause, Line, [],[], [&#123;tuple, Line, [&#123;atom, Line, error&#125;,&#123;atom, Line, lager_not_running&#125;]&#125;]&#125;]&#125;&#125;, []&#125;] &#125;, %% &#123;undefined, _, _&#125; -&gt; &#123;error, &#123;sink_not_configured, Sink&#125;&#125;; &#123;clause,Line, [&#123;tuple,Line, [&#123;atom,Line,undefined&#125;,&#123;var,Line,'_'&#125;,&#123;var,Line,'_'&#125;]&#125;], [], %% same trick as above to avoid linter error [&#123;call, Line, &#123;'fun', Line, &#123;clauses, [&#123;clause, Line, [],[], [&#123;tuple,Line, [&#123;atom,Line,error&#125;, &#123;tuple,Line,[&#123;atom,Line,sink_not_configured&#125;,&#123;atom,Line,SinkName&#125;]&#125;]&#125;]&#125;]&#125;&#125;, []&#125;] &#125;, %% &#123;SinkPid, _, &#123;Level, Traces&#125;&#125; when ... -&gt; lager:do_log/9; 图 1-1 总结一下，我们平时在用Erlang编程的时候应该不会涉及到自己编写parse_transform函数的需求，这个函数的功能非常强大，可以理解成是一个功能非常强大的宏，但是我觉得编写这个函数的话也会非常容易出错的，看下lager_transform.erl文件里面的代码就知道了。其实不单单lager使用了parse_transform函数的功能，ets也使用了这个功能，由于ets的select和match匹配的可读性实在太差了，所以可以使用ets:fun2ms/1模拟函数的写法来写匹配规则（当然不是真正的函数了，写起来有很多限制的），然后在编译的时候转化成select和match的匹配格式。","categories":[{"name":"Erlang深入","slug":"Erlang深入","permalink":"https://lintingbin2009.github.io/categories/Erlang深入/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"},{"name":"Lager","slug":"Lager","permalink":"https://lintingbin2009.github.io/tags/Lager/"}]},{"title":"Erlang垃圾回收","slug":"Erlang垃圾回收","date":"2017-05-21T05:02:42.000Z","updated":"2018-05-04T15:15:55.980Z","comments":true,"path":"2017/05/21/Erlang垃圾回收/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/21/Erlang垃圾回收/","excerpt":"","text":"由于Erlang的官方文档并没有介绍垃圾回收机制，本文参考一些论文和博客试着来解释下Erlang的内存回收机制，如果有存在错误，欢迎指正。 要解释Erlang的垃圾回收机制必须先知道Erlang的内存管理，在Erlang中存在三种架构来实现不同的内存管理方式，在之前的很长一段时间内用的都是process-centric的架构，现在不知道有没有改成Hybrid，没有找到相关的说明文档。接下来分别介绍下这三种架构： 图1 Hybrid架构内存组织 Process-centric 在该架构中，进程间通信需要复制消息，因此是O(n)操作，其中n是消息大小。内存碎片通常比较多。预计垃圾回收的时间和次数预计会很小（因为根集只需要堆叠的进程需要收集），并且在进程终止之后，其分配的内存区域可以在不间断的时间内被回收。在图1中相当于没有Shared Heap区域。 Communal 这种架构的最大的优点是非常快（O(1)）的通信，只需将指针传递给接收进程，由于消息共享，所以内存的需求也比较小，并且分散性低。缺点在于，必须将所有进程的堆栈作为根集的一部分（导致增加GC延迟），并且由于进程的数据在共享堆上交错而导致可能的缓存性能差。此外，这种架构不能很好地扩展到多线程或多处理器实现，因为需要锁定以便以并行设置分配和收集共享存储器区域。简单的说所有的消息都是共享的，进程内只存了指针。 Hybrid 这是一种尝试结合上述两个架构的优点的架构：进程间通信可能很快，并且进程本地堆的频繁收集的GC延迟预计会很小。对进程本地堆的垃圾收集不需要锁定，并且减少了共享堆上的压力，因此它不需要经常进行垃圾回收。而且，像Process-centric架构一样，当一个进程终止时，它的本地内存可以通过简单地将其附加到自由列表（free-list）来回收。图1是该架构的内存组织图。 由于之前Erlang里面默认采用的是Process-centric的架构，所以我们这边介绍Process-centric架构的内存回收方式，如果想要了解Hybrid的内存回收方式可以参考《Message Analysis-Guided Allocation and Low-Pause》这篇论文。 Process-centric架构由于没有Shared Heap，所以内存回收只涉及到进程的内存回收和Shared Area for Binaries的内存回收。 进程内存回收如图1所示，Erlang的进程和Linux的进程非常的像，由进程控制块（PCB）、堆（Stack）和栈（Heap）组成。 进程控制块：进程控制模块会保存一些关于进程的信息比如它在进程表中的标识符（PID）、当前状态（运行、等待）、它的注册名、初始和当前调用，同时PCB也会保存一些指向传入消息的指针，这些传入消息是存储在堆中连接表中的。 栈：它是一个向下增长的存储区，这个存储区保存输入和输出参数、返回地址、本地变量和用于执行表达式的临时空间。 堆：它是一个向上增长的存储区，这个存储区保存进程邮箱的物理消息，像列表、元组和Binaries这种的复合项以及比像浮点数这种一个机器字更大的对象。超过64机器字的二进制项不会存储在进程私有堆里，而是被存在图1的Shared Area for Binaries里面，进程堆维护一个列表（remembered list），该列表存储该进程指向Shared Area for Binaries区域的所有指针。 进程的内存回收机制采用的是分代标记清除(“stop the world” generational mark-sweep collector)的回收机制，通过这种机制把进程堆划分为了一个老年代（Old Generation）和新生代（Young Generation），使用minor collection对新生代进行垃圾回收，major collection进行整个堆的垃圾回收。进程垃圾回收的时候该进程会卡住，但是由于进程堆的大小一般都比较小所以回收的很快，而且这时候其他进程也在运行，所以垃圾回收不太会影响系统的响应能力。进程创建后的首次垃圾回收会使用major collection，后面在进程运行的过程中如果发现内存不够用的话会先使用minor collection进行回收，如果还是不能释放出足够的空间的话则会使用major collection进行回收，然后如果major collection还是不能释放出足够的空间的话，则会增加进程堆的大小。进程默认的min_heap_size的大小是233个字，进程堆大小的增长策略首先是斐波纳契序列增长，当堆的大小到达1.3M个字的时候堆每次只增长20%。 Shared Area for Binaries内存回收这个区域的内存回收采用的是标记清除的方法，在每个Binary的头部上会有一个数字，记录着这个Binary被引用了几次。在进程结束之后，进程堆中的remembered list的指针指向的Binary的引用次数会被相应的减1，同样在进程垃圾回收的时候如果发现remembered list中有可以被回收的指针，该指针所指向的Binary的引用次数也会被相应的减1，当一个Bianry的引用次数为0时，这个Binary就可以被回收。 建议通过了解Erlang垃圾回收的原理，可以在垃圾回收方面对系统进行一些调优，以及减少系统的内存使用量，以下是我总结的一些建议： 进程默认的min_heap_size的大小是233个字，如果能提前知道进程大概需要多少空间的话，在进程创建的时候指定min_heap_size的大小可以减少内存回收的次数。 由于进程内存回收是每个进程单独进行的，所以有些进程在申请了很多空间之后，很久没有运行，但是上次申请的空间其实有些已经没用了，如果进程一直不运行或者不触发回收，这部分内存就一直回收不了，这时候就需要手动的进行内存回收。建议可以定时执行以下代码进行内存回收： 12[erlang:garbage_collect(P) || P &lt;- erlang:processes(), &#123;status, waiting&#125; =:= erlang:process_info(P, status)], 我所在的项目在每次启动的时候都会进行大量的初始化，在项目启动成功后对所有进程一次手动回收也可以节省很多内存。 对一些重量级的操作可以spawn一个进程出来处理，当进程结束后该部分空间就能被完全回收了，比在原进程上面执行应该会好些。 之前看到一个开源项目在处理完一个请求后就对该进程进行一次手动回收，这个好像也是一个优化，因为一个请求过后再上来一个请求的话，可能需要秒的级别，进程在这段空闲时间进行一次回收不会影响系统的响应而且还能节省内存。","categories":[{"name":"Erlang深入","slug":"Erlang深入","permalink":"https://lintingbin2009.github.io/categories/Erlang深入/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"},{"name":"GC","slug":"GC","permalink":"https://lintingbin2009.github.io/tags/GC/"}]},{"title":"编程语言的垃圾回收机制简介","slug":"编程语言的垃圾回收机制简介","date":"2017-05-09T03:00:47.000Z","updated":"2018-05-04T15:15:55.989Z","comments":true,"path":"2017/05/09/编程语言的垃圾回收机制简介/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/09/编程语言的垃圾回收机制简介/","excerpt":"","text":"现在的编程语言中大多都包括了垃圾回收（Garbage Collection）机制，垃圾回收机制是一种自动的内存管理机制，当计算机内存中的一个对象不再需要被使用时，就会自动的让出这块内存。在早期的C/C++编程语言中，程序员需要自己手动申请和释放内存，而因在编程的过程中往往会经常忘记释放那些不再使用的内存，进而造成内存泄漏。垃圾回收机制可以大大减轻程序员的负担，减少程序员犯错的机会。垃圾回收机制最早起源于LISP语言，目前的大多数高级语言都支持内存回收机制，比如：PHP、Java、C#、Erlang等等。 垃圾回收算法的原理： 推算出某个对象在未来的程序运行中将不再会被访问。 将这些对象占用的内存回收。 收集器实现 引用计数收集器 最早期的垃圾回收实现方法，通过对数据存储的物理空间附加多一个计数器空间，当有其他数据与其相关时则加一，反之相关解除时减一，定期检查各储存对象的计数器，为零的话则认为已经被抛弃而将其所占物理空间回收。是最简单的实现，但存在无法回收循环引用的存储对象的缺陷。 跟踪收集器 近现代的垃圾回收实现方法，通过定期对若干根储存对象开始遍历，对整个程序所拥有的储存空间查找与之相关的存储对象和没相关的存储对象进行标记，然后将没相关的存储对象所占物理空间回收。 回收算法主要的回收算法可以分为以下几类： 标记－清除 先暂停整个程序的全部运行线程，让回收线程以单线程进行扫描标记，并进行直接清除回收，然后回收完成，恢复运行线程。会导致大量零碎的空闲空间碎片，导致大容量对象不容易获得连续的内存空间，而造成空间浪费。 标记－压缩 和“标记－清除”相似，不同的是，回收期间同时会将保留的存储对象搬运汇集到连续的内存空间。从而集成空闲空间。 复制 需要程序将所拥有的内存空间分成两个部分。程序运行所需的存储对象先存储在其中一个分区（定义为“分区0”）。同样暂停整个程序的全部运行线程后，进行标记后，回收期间将保留的存储对象搬运汇集到另一个分区（定义为“分区1”），完成回收，程序在本次回收后将接下来产生的存储对象会存储到“分区1”。在下一次回收时，两个分区的角色对调。 增量回收器 需要程序将所拥有的内存空间分成若干分区。程序运行所需的存储对象会分布在这些分区中，每次只对其中一个分区进行回收操作，从而避免程序全部运行线程暂停来进行回收，允许部分线程在不影响回收行为而保持运行，并且降低回收时间，增加程序响应速度。 分代 由于“复制”算法对于存活时间长，大容量的储存对象需要耗费更多的移动时间，和存在储存对象的存活时间的差异。需要程序将所拥有的内存空间分成若干分区，并标记为年轻代空间和年老代空间。程序运行所需的存储对象会先存放在年轻代分区，年轻代分区会较为频密进行较为激进垃圾回收行为，每次回收完成幸存的存储对象内的寿命计数器加一。当年轻代分区存储对象的寿命计数器达到一定阈值或存储对象的占用空间超过一定阈值时，则被移动到年老代空间，年老代空间会较少运行垃圾回收行为。一般情况下，还有永久代的空间，用于涉及程序整个运行生命周期的对象存储，例如运行代码、数据常量等，该空间通常不进行垃圾回收的操作。 通过分代，存活在局限域，小容量，寿命短的存储对象会被快速回收；存活在全局域，大容量，寿命长的存储对象就较少被回收行为处理干扰。 实现上面是垃圾回收的基本算法，有些编程语言的垃圾回收机制会使用上面的算法然后再自己进行改造优化性能。比如Erlang就同时使用分代回收和标记清除的算法来实现垃圾回收机制。","categories":[{"name":"基本概念","slug":"基本概念","permalink":"https://lintingbin2009.github.io/categories/基本概念/"}],"tags":[{"name":"GC","slug":"GC","permalink":"https://lintingbin2009.github.io/tags/GC/"}]},{"title":"Erlang代码热更新","slug":"Erlang代码热更新","date":"2017-05-04T15:30:18.000Z","updated":"2018-05-04T15:15:55.979Z","comments":true,"path":"2017/05/04/Erlang代码热更新/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/04/Erlang代码热更新/","excerpt":"","text":"通过上一篇的文章Erlang动态代码载入小实验，我们可以了解到Erlang的热更机制，在Erlang里面会维护两个版本的代码。在新版本载入的时候如果有进程在老版本运行的话，运行那些内部调用的函数（只通过函数名调用的）代码将不会被更新，只有那些通过M:F格式调用的内部函数才能热更。举个例子： 123456789%% 如果代码在loop上执行的话，有两种情况loop() -&gt; io:format(\"v1 ~n\"), %% 这条语句这种情况不能热更 loop().loop() -&gt; io:format(\"v1 ~n\"), %% 这条语句这种情况可以热更 ?MODULE:loop(). 在Erlang里面有分本地调用（local calls）和外部调用（external calls），本地调用的函数名是不需要被导出的。本地调用的格式是Fun(Args)，外部调用的格式是M:F(Args)。 Erlang运行时会保存一份代码的两个版本，所有本地调用的函数地址都会指向程序运行时最初的那个版本（如上面例子的情况一），而所有外部调用的函数地址都会指向最新的版本（如上面例子的情况二）。所以如果想要让代码能够热更新的话，需要使用外部调用的格式。 在我们项目中一般热更的流程是先：code:soft_purge(ModName)或者code:purge(ModName)然后再code:load_file(ModName)进行热更，针对这一热更流程我之前一直存在两个问题，最近仔细研究下才找到了答案，分别是以下这两个问题： 为什么load_file之前要先soft_purge或者purge一下呢？ 这个是load_file函数的问题，如果在load_file执行的时候，本身要热更的模块就有一个老的版本的代码存在的话，load_file就会返回一个not_purged的错误代码，导致新版本不能正常的载入。如果load_file执行自动删除最老版本的话，就不需要purge了（像在Erlang Shell里面执行c(ModName)一样）。当然如果一个模块从来都没有热更过的话（在系统里面只有一个版本），直接使用load_file是没有问题的，不过之后就要先purge再load_file了。 soft_purge和purge有什么不同吗？ 函数的功能上是有所不同的，但是在我们项目的使用中几乎是没有什么不同的。soft_purge和purge的函数的功能区别是如果清理的模块的老的版本中有进程在上面运行的话，purge就会杀掉进程，然后把老的版本给清理掉，soft_purge则会清理失败。热更的时候是先执行purge然后再loadfile，由于进程一般都是在当前的版本上面执行，这时候老的版本上面不会有进程在运行，所以执行purge和soft_purge是一样的，如果真的想要热更的时候把进程杀掉的话应该执行purge/soft_purge-&gt;loadfile-&gt;purge。 以上就是我对Erlang代码热更的总结～","categories":[{"name":"Erlang入门教程","slug":"Erlang入门教程","permalink":"https://lintingbin2009.github.io/categories/Erlang入门教程/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"Erlang动态代码载入小实验","slug":"Erlang动态代码载入小实验","date":"2017-05-01T15:19:04.000Z","updated":"2018-05-04T15:15:55.980Z","comments":true,"path":"2017/05/01/Erlang动态代码载入小实验/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/01/Erlang动态代码载入小实验/","excerpt":"","text":"下面的内容为《Erlang程序设计（第2版）》8.10节的内容，这个动态代码载入的小实验非常的简单生动，通过这个小实验能够充分理解Erlang代码的载入机制。 动态代码载入是内建于Erlang核心的最惊人特性之一。它的美妙之处在于你无需了解后台的运作就能顺利实现它。 它的思路很简单：每当调用 someModule:someFunction(…)时，调用的总是最新版模块里的最新版函数，哪怕当代码在模块里运行时重新编译了该模块也是如此。 如果在a循环调用b时重新编译了b，那么下一次a调用b时就会自动调用新版的 b 。如果有许多不同进程正在运行而它们都调用了b，那么当b被重新编译后，所有这些进程就都会调用新版的b 。为了了解它的工作原理，我们将编写两个小模块：a和b。 12345%% b.erl-module(b).-export([x/0]).x() -&gt; 1. 1234567891011121314151617%% a.erl-module(a).-compile(export_all).start(Tag) -&gt; spawn(fun() -&gt; loop(Tag) end).loop(Tag) -&gt; sleep(), Val = b:x(), io:format(\"Vsn1 (~p) b:x() = ~p~n\", [Tag, Val]), loop(Tag).sleep() -&gt; receive after 3000 -&gt; true end. 现在可以编译a和b，然后启动两个a进程。 1&gt; c(a). {ok,a} 2&gt; c(b). {ok,b} 3&gt; a:start(one). &lt;0.70.0&gt; Vsn1 (one) b:x() = 1 Vsn1 (one) b:x() = 1 4&gt; a:start(two). &lt;0.72.0&gt; Vsn1 (one) b:x() = 1 Vsn1 (two) b:x() = 1 这些a进程休眠3秒钟后唤醒并调用b:x()，然后打印出结果。现在进入编辑器，把模块b改 成下面这样： 1234-module(b).-export([x/0]).x() -&gt; 2. 然后在shell里面重新编译b。这是现在所发生的： 5&gt; c(b). {ok,b} Vsn1 (one) b:x() = 2 Vsn1 (two) b:x() = 2 两个原版的a仍然在运行，但现在它们调用了新版的b。所以在模块a里调用b:x()时，实际上是在调用“b的最新版”。我们可以随心所欲地多次修改并重新编译b，而所有调用它的模块无需特别处理就会自动调用新版的b。 现在已经重新编译了b，那么如果我们修改并重新编译a会发生什么？来做个试验，把a改成下面这样： 12345678910111213141516-module(a).-compile(export_all).start(Tag) -&gt; spawn(fun() -&gt; loop(Tag) end).loop(Tag) -&gt; sleep(), Val = b:x(), io:format(\"Vsn2 (~p) b:x() = ~p~n\", [Tag, Val]), loop(Tag).sleep() -&gt; receive after 3000 -&gt; true end. 现在编译并启动a。 6&gt; c(a). {ok,a} Vsn1 (two) b:x() = 2 Vsn1 (one) b:x() = 2 Vsn1 (two) b:x() = 2 7&gt; a:start(three). &lt;0.84.0&gt; Vsn1 (two) b:x() = 2 Vsn1 (one) b:x() = 2 Vsn2 (three) b:x() = 2 Vsn1 (two) b:x() = 2 有趣的事情发生了。启动新版的a后，我们看到了新版正在运行。但是，那些运行最初版a的现有进程仍然在正常地运行旧版的a。 现在可以试着再次修改b。 1234-module(b).-export([x/0]).x() -&gt; 3. 我们将在shell里重新编译b，观察会发生什么。 8&gt; c(b). {ok,b} Vsn1 (one) b:x() = 3 Vsn2 (three) b:x() = 3 Vsn1 (two) b:x() = 3 现在新旧版本的a都调用了b的最新版。 最后，再次修改a（这是第三次修改a了）。 12345678910111213141516-module(a).-compile(export_all).start(Tag) -&gt; spawn(fun() -&gt; loop(Tag) end).loop(Tag) -&gt; sleep(), Val = b:x(), io:format(\"Vsn3 (~p) b:x() = ~p~n\", [Tag, Val]), loop(Tag).sleep() -&gt; receive after 3000 -&gt; true end. 现在，当我们重新编译a并启动一个新版的a时，就会看到以下输出： 9&gt; c(a). {ok,a} Vsn2 (three) b:x() = 3 Vsn2 (three) b:x() = 3 Vsn2 (three) b:x() = 3 Vsn2 (three) b:x() = 3 10&gt; a:start(four). &lt;0.96.0&gt; Vsn2 (three) b:x() = 3 Vsn3 (four) b:x() = 3 Vsn2 (three) b:x() = 3 Vsn3 (four) b:x() = 3 Vsn2 (three) b:x() = 3 这段输出里的字符串是由两个最新版本的a（第2版和第3版）生成的，而那些运行第1版a代码的进程已经消失了。 在任一时刻，Erlang允许一个模块的两个版本同时运行：当前版和旧版。重新编译某个模块时，任何运行旧版代码的进程都会被终止，当前版成为旧版，新编译的版本则成为当前版。可以把这想象成一个带有两个版本代码的移位寄存器。当添加新代码时，最老的版本就被清除了。一些进程可以运行旧版代码，与此同时，另一些则可以运行新版代码。","categories":[{"name":"Erlang入门教程","slug":"Erlang入门教程","permalink":"https://lintingbin2009.github.io/categories/Erlang入门教程/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"Erlang中catch和try...catch的区别","slug":"Erlang中catch和try...catch的区别","date":"2017-05-01T08:06:51.000Z","updated":"2018-05-04T15:15:55.977Z","comments":true,"path":"2017/05/01/Erlang中catch和try...catch的区别/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/01/Erlang中catch和try...catch的区别/","excerpt":"","text":"在Erlang的错误处理中，catch并不是try…catch的缩写，try…catch和catch是不同的。下面我将通过一个例子来区别出他们的不同，为以后的使用做一个参考。 123456789101112131415161718192021222324%% exception_test.erl 代码文件-module(exception_test).-compile(export_all).generate_exception(1) -&gt; a;generate_exception(2) -&gt; throw(a);generate_exception(3) -&gt; error(a);generate_exception(4) -&gt; exit(a);generate_exception(5) -&gt; &#123;'EXIT', a&#125;.test_use_catch() -&gt; [&#123;I, catch generate_exception(I)&#125; || I &lt;- lists:seq(1, 5)].test_user_try_catch() -&gt; [begin try generate_exception(I) of NormalRes -&gt; &#123;I, normal, NormalRes&#125; catch ErrorType : Error -&gt; &#123;I, exception, ErrorType, Error&#125; end end || I &lt;- lists:seq(1, 5)]. 12345678910111213141516%% 执行exception_test:test_use_catch().函数的返回结果[&#123;1,a&#125;, &#123;2,a&#125;, &#123;3, &#123;'EXIT',&#123;a,[&#123;exception_test,generate_exception,1, [&#123;file,\"exception_test.erl\"&#125;,&#123;line,7&#125;]&#125;, &#123;exception_test,'-test_use_catch/0-lc$^0/1-0-',1, [&#123;file,\"exception_test.erl\"&#125;,&#123;line,12&#125;]&#125;, &#123;exception_test,'-test_use_catch/0-lc$^0/1-0-',1, [&#123;file,\"exception_test.erl\"&#125;,&#123;line,12&#125;]&#125;, &#123;erl_eval,do_apply,6,[&#123;file,\"erl_eval.erl\"&#125;,&#123;line,674&#125;]&#125;, &#123;shell,exprs,7,[&#123;file,\"shell.erl\"&#125;,&#123;line,686&#125;]&#125;, &#123;shell,eval_exprs,7,[&#123;file,\"shell.erl\"&#125;,&#123;line,641&#125;]&#125;, &#123;shell,eval_loop,3,[&#123;file,\"shell.erl\"&#125;,&#123;line,626&#125;]&#125;]&#125;&#125;&#125;, &#123;4,&#123;'EXIT',a&#125;&#125;, &#123;5,&#123;'EXIT',a&#125;&#125;] 123456%% 执行exception_test:test_user_try_catch().函数的返回结果[&#123;1,normal,a&#125;, &#123;2,exception,throw,a&#125;, &#123;3,exception,error,a&#125;, &#123;4,exception,exit,a&#125;, &#123;5,normal,&#123;'EXIT',a&#125;&#125;] 通过上面的列子我们可以看到，如果使用标准的try…catch来处理错误的话，调用者是可以正确的识别出错误，然后对错误进行相应的处理的。 但是如果用的是catch来处理错误的话，情况是不能乐观的，使用catch处理错误，exception(1)和exception(2)返回的结果是一样的，exception(4)和exception(5)返回的结果是一样的。catch在处理throw的时候只是简单的把throw的内容给返回，在处理exit的时候会返回一个tuple是带’EXIT’和exit里面的内容的结果，在处理error的时候会把堆栈给打印出来（这点比较人性化）。 所以大家在使用catch的时候要注意catch的返回值，正常的情况下还是推荐使用try…catch来处理错误，不然很容易就会掉到坑里面的。","categories":[{"name":"Erlang入门教程","slug":"Erlang入门教程","permalink":"https://lintingbin2009.github.io/categories/Erlang入门教程/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"吴恩达机器学习课程练习实现","slug":"吴恩达机器学习课程练习实现","date":"2017-05-01T04:30:06.000Z","updated":"2018-05-04T15:15:55.988Z","comments":true,"path":"2017/05/01/吴恩达机器学习课程练习实现/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/01/吴恩达机器学习课程练习实现/","excerpt":"","text":"由于当前机器学习大火，让我对机器学习产生浓厚的兴趣，所以我就上网查了下机器学习的入门教程，大多数的人还是比较推荐吴恩达老师的机器学习课程的。所以我就在Coursera上面学习了吴恩达的机器学习课程，现在已经顺利的毕业了。 在Coursera上面每学完一小节课程都会有相应的练习，系统会自动对你提交的练习进行打分，要达到指定的分数才能顺利通过，这点Coursera的体验还是做的比较好的，但是由于国内网络的原因，在Coursera上面看视频，如果不使用科学上网的话，有时候是看不了的。 我做练习用的是Octave，用Octave提交代码的时候会报错，需要在练习中进行以下的代码替换: lib/submitWithConfiguration.m 文件66行 1responseBody = urlread(submissionUrl, &apos;post&apos;, params); 替换成： 1[code, responseBody] = system(sprintf(&apos;echo jsonBody=%s | curl -k -X POST -d @- %s&apos;, body, submissionUrl)); 最后附上我的练习代码，有需要的可以自取，如果对你有帮助的话，记得给我星星哈～ 代码地址：https://github.com/lintingbin2009/machine-learning-ex","categories":[{"name":"代码","slug":"代码","permalink":"https://lintingbin2009.github.io/categories/代码/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://lintingbin2009.github.io/tags/机器学习/"}]},{"title":"剑指offer——C语言实现","slug":"剑指offer——C语言实现","date":"2017-05-01T04:09:54.000Z","updated":"2018-05-04T15:15:55.988Z","comments":true,"path":"2017/05/01/剑指offer——C语言实现/","link":"","permalink":"https://lintingbin2009.github.io/2017/05/01/剑指offer——C语言实现/","excerpt":"","text":"之前实习的时候为了能够比较顺利的找到一个实习，特意练习了下代码能力，期间看了挺多的算法书籍，然后把剑指offer书本上的那些练习大部分都自己用C语言实现了一遍。 这些练习的代码都已经传到了Github上面了，地址是： https://github.com/lintingbin2009/C-language/tree/master/%E5%89%91%E6%8C%87offer 有需要的可以自取，如果觉得对你有帮助的话，记得给我个星星哈～","categories":[{"name":"代码","slug":"代码","permalink":"https://lintingbin2009.github.io/categories/代码/"}],"tags":[{"name":"剑指offer","slug":"剑指offer","permalink":"https://lintingbin2009.github.io/tags/剑指offer/"},{"name":"C语言","slug":"C语言","permalink":"https://lintingbin2009.github.io/tags/C语言/"}]},{"title":"Erlang数据类型","slug":"Erlang数据类型","date":"2017-04-30T03:23:06.000Z","updated":"2018-05-04T15:15:55.981Z","comments":true,"path":"2017/04/30/Erlang数据类型/","link":"","permalink":"https://lintingbin2009.github.io/2017/04/30/Erlang数据类型/","excerpt":"","text":"Erlang提供的数据类型，包括以下几种： 基本类型 数字(Number) 数字类型又包含整数(integers)和浮点数(floats)类型，其中整数是精确的而且是支持大数的，小数是满足IEEE754规则的64位浮点数。Erlang支持ASCII或者Unicode转换成整数值，同时支持整数不同进制的表示。（‘%’后的内容为注释） 1&gt; $a. %% ASCII表中的a是97 97 2&gt; $哈. 21704 3&gt; $\\n. 10 4&gt; 2#100. %% 用100表示的二进制是4 4 5&gt; 4#100. 16 6&gt; 16#100. 256 原子(Atom) 原子可以理解为一个不可变的常量，必须以小写字母开头，如果要以大写、下划线或者其他的特殊字符开头，必须加上单引号。原子在Erlang里面是存在一张表上面的，原子的个数有上限，大概是在一百万个左右。 test ‘Myhome’ ‘_hero’ 位串和二进制(Bit Strings and Binaries) 在大多数情况下，二进制型里的位数都会是8的整数倍，因此对应一个字节串。如果位数不是8的整数倍，就称这段数据为位串（bitstring）。所以当我们说位串时，是在强调数据里的位数不是8的整数倍。位语法是一种表示法，用于从二进制数据里提取或加入单独的位或者位串。当你编写底层代码，以位为单位打包和解包二进制数据时，就会发现位语法是极其有用的。 1&gt; &lt;&lt;257,1,2,3,5&gt;&gt;. %%二进制型的元素如果大于8位的会自动截断，257截断成1 &lt;&lt;1,1,2,3,5&gt;&gt; 2&gt; &lt;&lt;0:7,1:2&gt;&gt;. %%二进制型位数如果不是8的整数倍就会产生位串，这边多了1位1 &lt;&lt;0,1:1&gt;&gt; 3&gt; &lt;&lt;0:3,0:4,1:2&gt;&gt;. &lt;&lt;0,1:1&gt;&gt; 引用(Reference) 可以通过make_ref/0函数来创建一个引用，引用在Erlang程序运行时调用make_ref函数产生的是全局唯一的。比如timer模块在创建一个定时任务的时候通常会返回一个引用，可以通过这个引用来取消定时任务。 函数(Fun) 函数在Erlang里面也算是一种数据类型，通过给变量绑定函数，可以通过变量名来执行函数。 1&gt; Fun = fun(X) -&gt; X * X end. #Fun&lt;erl_eval.6.50752066&gt; 2&gt; Fun(9). 81 端口标识符(Port Identifier) 端口用于与外界通信，由通过函数open_port/2来创建。消息可以通过端口进行收发，但是这些消息必须遵守所谓“端口协议”(port protocol)的规则。 进程标识符(Pid) 当创建一个进程的时候会产生一个进程标识符，可以通过这个进程标识符和进程进行通讯。 1&gt; Process1 = spawn(fun() -&gt; receive X -&gt; io:format(“recv ~p, bye~n”, [X]) end end). &lt;0.34.0&gt; %% 创建一个进程等待接收消息 2&gt; Process1 ! my_test. %% 给进程发消息 recv my_test, bye my_test 复合类型为了方便定义以下的这些复合类型，我把上述的所有基本类型都称为Term。 元组(Tuple) 元组类似于C语言里面的结构体(Struct)，是由固定数量的元素组成的复合数据类型，可以定义成如下结构： {Term1, Term2, …, TermN} 可以通过模式匹配或者element/2函数来提取元组里面元素的值，通过setelement/3来设置元组里面元素的值，size可以取元组里面元素的个数。 1&gt; P = {adam,24,{july,29}}. {adam,24,{july,29}} 2&gt; element(1,P). adam 3&gt; element(3,P). {july,29} 4&gt; P2 = setelement(2,P,25). {adam,25,{july,29}} 5&gt; size(P). 3 6&gt; {adam, Old, {Month, Day}} = P. {adam,24,{july,29}} 7&gt; Old. 24 映射组(Map) 映射组是一个由多个Key-Vaule结构组成的符合数据类型，可以定义为如下结构： #{Key1=&gt;Value1, Key2=&gt;Value2, …, KeyN=&gt;ValueN} 其中Key、Value都是Term 可以通过maps模块提供的一些函数对映射组进行操作 1&gt; M1 = #{name=&gt;adam,age=&gt;24,date=&gt;{july,29}}. #{age =&gt; 24,date =&gt; {july,29},name =&gt; adam} 2&gt; maps:get(name,M1). adam 3&gt; maps:get(date,M1). {july,29} 4&gt; M2 = maps:update(age,25,M1). #{age =&gt; 25,date =&gt; {july,29},name =&gt; adam} 5&gt; map_size(M). 3 6&gt; map_size(#{}). 0 列表(List) 列表类似于其他语言里面的数组，是由可变数量的元素组成的复合数据结构，可以定义成如下结构： [Term1, Term2, …, TermN] 在Erlang里面，列表由一个头和一个尾组成，空列表也是一个列表。所以列表也可以有一个递归的定义 List = [Term| List] | [] [] 是一个列表, 因此 [c|[]] 是一个列表, 因此 [b|[c|[]]] 是一个列表, 因此[a|[b|[c|[]]]] 是一个列表, 或者简写为 [a,b,c] lists模块可以提供大量函数对列表进行操作： 1&gt; L = [3,3,4,2,1,2,34]. [3,3,4,2,1,2,34] 2&gt; length(L). 7 3&gt; lists:sort(L). [1,2,2,3,3,4,34] 4&gt; lists:reverse(L). [34,2,1,2,4,3,3] 其他类型(不算数据类型) 字符串(String) 字符串用一对双引号括起来，但不算是Erlang中的数据类型。字符串仅仅是列表的一个缩写，比如：字符串”hello”是列表[$h,$e,$l,$l,$o]的一个缩写。两个相邻的字符串在编译的时候连接成一个字符串，不会造成任何运行时开销。 1&gt; “hello” “ “ “world”. “hello world” 记录(Record) 记录其实就是元组的另一种形式。通过使用记录，可以给元组里的各个元素关联一个名称。对记录的处理是在编译的时候完成的，在运行时是不会有记录的，可以把记录理解成是元组的一种语法糖。 12345-module(person).-export([new/2]).-record(person, &#123;name, age&#125;).new(Name, Age) -&gt; #person&#123;name=Name, age=Age&#125;. 1&gt; person:new(ernie, 44). {person,ernie,44} 布尔类型(Boolean) 在Erlang中没有Boolean类型。而是用原子true和false来表示布尔值。 1&gt; 2 =&lt; 3. true 2&gt; true or false. true 类型转换Erlang提供了一些内置的类型转换函数，可以方便地进行类型转换，下面是一些类型转换的例子： 1&gt; atom_to_list(hello). “hello” 2&gt; list_to_atom(“hello”). hello 3&gt; binary_to_list(&lt;&lt;”hello”&gt;&gt;). “hello” 4&gt; binary_to_list(&lt;&lt;104,101,108,108,111&gt;&gt;). “hello” 5&gt; list_to_binary(“hello”). &lt;&lt;104,101,108,108,111&gt;&gt; 6&gt; float_to_list(7.0). “7.00000000000000000000e+00” 7&gt; list_to_float(“7.000e+00”). 7.0 8&gt; integer_to_list(77). “77” 9&gt; list_to_integer(“77”). 77 10&gt; tuple_to_list({a,b,c}). [a,b,c] 11&gt; list_to_tuple([a,b,c]). {a,b,c} 12&gt; term_to_binary({a,b,c}). &lt;&lt;131,104,3,100,0,1,97,100,0,1,98,100,0,1,99&gt;&gt; 13&gt; binary_to_term(&lt;&lt;131,104,3,100,0,1,97,100,0,1,98,100,0,1,99&gt;&gt;). {a,b,c} 14&gt; binary_to_integer(&lt;&lt;”77”&gt;&gt;). 77 15&gt; integer_to_binary(77). &lt;&lt;”77”&gt;&gt; 16&gt; float_to_binary(7.0). &lt;&lt;”7.00000000000000000000e+00”&gt;&gt; 17&gt; binary_to_float(&lt;&lt;”7.000e+00&gt;&gt;”). 7.0","categories":[{"name":"Erlang入门教程","slug":"Erlang入门教程","permalink":"https://lintingbin2009.github.io/categories/Erlang入门教程/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"为什么使用Erlang?","slug":"为什么使用Erlang","date":"2017-04-29T14:00:09.000Z","updated":"2018-05-04T15:15:55.986Z","comments":true,"path":"2017/04/29/为什么使用Erlang/","link":"","permalink":"https://lintingbin2009.github.io/2017/04/29/为什么使用Erlang/","excerpt":"","text":"主要特性如果问我觉得Erlang最重要的特性是什么的话，我觉得应该是并发。 并发能够带来的好处是不言而喻的，比如： 性能 现在的计算机由于主频的限制，都在往多核的方式发展，有些比较高端的机器甚至有几十个核心。如果编写的程序都是顺序运行的话将会严重浪费多核计算机的计算能力。Erlang本身是面向并发编程的，如果把之前在单核机器上面跑的Erlang程序放到多核机器上面跑的话，性能将会极大的提高。 扩展性 如果在一台机器上面运行Erlang程序还不能满足性能的要求的话，可以简单的升级机器的CPU核心个数，甚至可以经过简单的改造把不同的进程分配到不同的机器上面运行，通过水平扩展方式来满足高并发的业务需求。 容错性 Erlang内部实现的进程是相互独立的，一个进程的崩溃并不会影响到另外一个进程的运行，同时Erlang内部还OTP框架来保证系统的容错性。 清晰性 Erlang世界观和现实的世界是一样的，在大多数的编程语言里面事情都是顺序发生的，但是在Erlang的世界里面所有的事件都是并发的，在编写程序的时候能够比较清晰的把现实世界事件的并行发生的的特性映射到Erlang的并发编程上面。 简介快速介绍下Erlang比较与众不认同的特性： Erlang Shell 在编写Erlang程序的过程中会有很多时间花费在Erlang Shell里面，Erlang Shell类似于Linux的Bash，开发者能在Erlang Shell里面运行表达式，通过这种交互方式，开发者能够在Erlang Shell里面调试正在运行的Erlang程序（包括远程的Erlang程序）。 = 操作符 在一般的编程语言里面，=表示赋值操作，一个变量能够被多次赋值。但是在Erlang里面变量是不可变的，一旦通过=绑定之后，该变量的值就不能发生改变了，重复绑定会导致异常。 变量和原子 所有Erlang的变量都是以大写字母开头的，比如：One、This和My_baby这些都是变量。以小写字母开头的则是符号常量（被称为原子：atom），比如：person、one和hello_world。 进程 Erlang的进程是Erlang虚拟机内部自己实现的进程，非常轻量级，刚开始创建的时候每个进程的大小也就2KB左右，1GB的内存就可以创建50万个进程。同时进程间没有共享内存，进程间的通信通过消息转发实现。 总结Erlang的特性决定了它是一门比较另类的语言，相信第一次见到它的人会觉得很吃惊，世界上居然会有这样的一门语言。但正是由于这些看似奇怪的特性，让Erlang能够在当今多核的时代充分的发挥它的能力。","categories":[{"name":"Erlang入门教程","slug":"Erlang入门教程","permalink":"https://lintingbin2009.github.io/categories/Erlang入门教程/"}],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"https://lintingbin2009.github.io/tags/Erlang/"}]},{"title":"使用HEXO在Github上搭建个人博客","slug":"使用HEXO在github上搭建个人博客","date":"2017-04-29T07:40:51.000Z","updated":"2018-05-04T15:15:55.987Z","comments":true,"path":"2017/04/29/使用HEXO在github上搭建个人博客/","link":"","permalink":"https://lintingbin2009.github.io/2017/04/29/使用HEXO在github上搭建个人博客/","excerpt":"","text":"在平时的工作中经常会遇到一些问题，在解决问题的时候如果能够及时记录下来是最好不过的，所以一直想维护一个自己的博客。虽然国内有各种技术博客（比如：CSDN，博客园）之类的第三方博客平台，但是作为一个程序员，不搭建一个自己的博客感觉不够酷。所以我就选择使用HEXO在Github上面搭建自己的个人博客。 下面的安装教程都是在Window x64的环境下进行的 安装步骤 申请Github账户 由于博客是要搭建在Github上面的，所有必须要有一个Github账号来上传代码，这样才能最终显示自己的博客内容。在建立完Github账号后，需要创建一个Repositories，这个Repositories的名字的格式是:your_user_name.github.io这样的。 安装Git软件 有了Github账号后还需要有软件能把本地的代码上传到Github上面，所就安装Git软件，安装Git也非常简单，直接下一步就行了。 安装NodeJs 由于Hexo是基于NodeJs的框架，所以使用Hexo前要先安装NodeJs，安装NodeJs也非常简单，只需要下载软件，点下一步就行了。现在新的版本的NodeJs，会同时安装npm（Node包管理软件），所以安装起来非常简单。 安装Hexo 把上面的软件都安装好了之后就可以开始安装Hexo了，打开window的终端，在终端中输入下面的命令开始安装Hexo 1npm install -g hexo 使用步骤 初始化 创建一个文件夹，如：MyBlog之类，然后进到MyBlog文件夹下执行以下初始化命令 1hexo init 到了这一步之后，Hexo算初始化完成，可以正常的使用了。 生成静态页面 继续在MyBlog目录下执行如下命令，生成静态页面 1hexo generate // 简写 hexo g 本地启动 启动本地服务，进行文章预览调试，命令： 1hexo server // 动态启动，有修改发生会自动检测，简写 hexo s 然后在浏览器输入 http://localhost:4000 就可以看到博客的页面，当然也在服务器启动的时候加上-p来指定自己想要的端口 部署步骤 安装 hexo-deployer-git 1npm install hexo-deployer-git --save 配置部署环境 在MyBlog的目录下会有一个_config.yml的文件，该文件为Hexo项目的配置文件，打开该文件然后把deploy部分改成下列格式 1234deploy: type: git repository: https://github.com/lintingbin2009/lintingbin2009.github.io.git // lintingbin2009替换成你自己的名字 branch: master 开始部署 1hexo deploy 部署完成之后就可以使用your_username.github.io来访问你的个人博客了, 之后的部署命令应该是 123hexo cleanhexo generatehexo deploy 总结总的来说用Hexo在Github上搭建个人博客还是比较简单的，当然这边只是涉及到最简单的搭建，还没有涉及到主题的更换、评论系统，统计系统。更多关于Hexo的使用文档可以浏览Hexo的中文官网，里面有详细的使用教程和很多可选的精美主题。","categories":[{"name":"教程","slug":"教程","permalink":"https://lintingbin2009.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://lintingbin2009.github.io/tags/Hexo/"},{"name":"Github","slug":"Github","permalink":"https://lintingbin2009.github.io/tags/Github/"}]}]}